{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":87433,"databundleVersionId":10051766,"sourceType":"competition"},{"sourceId":3842332,"sourceType":"datasetVersion","datasetId":2286778},{"sourceId":9985415,"sourceType":"datasetVersion","datasetId":6144904},{"sourceId":186011,"sourceType":"modelInstanceVersion","modelInstanceId":150617,"modelId":173092},{"sourceId":188053,"sourceType":"modelInstanceVersion","modelInstanceId":160318,"modelId":182694},{"sourceId":188056,"sourceType":"modelInstanceVersion","modelInstanceId":160321,"modelId":182697},{"sourceId":194160,"sourceType":"modelInstanceVersion","modelInstanceId":161064,"modelId":183449},{"sourceId":196006,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":161064,"modelId":183449},{"sourceId":196165,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":161064,"modelId":183449}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"СТРУКТУРА:\n1. Предоставленный бейзлайн\n2. Новый бейзлайн (ResNet)\n3. WAVEGAN\n   ","metadata":{}},{"cell_type":"markdown","source":"# Прописываем пути","metadata":{}},{"cell_type":"code","source":"import os\n\ntrain_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train\"\ndev_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev\"\neval_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_eval\"\n\nprint(\"Train files:\", os.listdir(train_path)[:5])\nprint(\"Dev files:\", os.listdir(dev_path)[:5])\nprint(\"Eval files:\", os.listdir(eval_path)[:5])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprotocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols\"\n\n\nwith open(os.path.join(protocol_path, \"ASVspoof2019.LA.cm.train.trn.txt\")) as f:\n    print(\"Train protocol example:\", f.readline())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision torchaudio librosa tqdm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ОЦЕНКА предоставленного бейзлайна Res2TCNGuard","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/input/safespeak-baseline/requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:23:23.020587Z","iopub.execute_input":"2024-11-22T18:23:23.020954Z","iopub.status.idle":"2024-11-22T18:23:26.231167Z","shell.execute_reply.started":"2024-11-22T18:23:23.020921Z","shell.execute_reply":"2024-11-22T18:23:26.230185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch_optimizer\n# !pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T09:28:08.103411Z","iopub.execute_input":"2024-11-23T09:28:08.103712Z","iopub.status.idle":"2024-11-23T09:28:17.754813Z","shell.execute_reply.started":"2024-11-23T09:28:08.103686Z","shell.execute_reply":"2024-11-23T09:28:17.753993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python /kaggle/input/safespeak-baseline/eval.py \\\n# --config /kaggle/input/safespeak-baseline/configs/config_res2tcnguard.json \\\n# --eval_label_path /kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt \\\n# --eval_path_flac /kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_eval\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:23:45.862568Z","iopub.execute_input":"2024-11-22T18:23:45.862871Z","iopub.status.idle":"2024-11-22T18:58:10.681191Z","shell.execute_reply.started":"2024-11-22T18:23:45.862841Z","shell.execute_reply":"2024-11-22T18:58:10.680233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Прогон предоставленного бейзлайна на данных соревнования","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\n\ntest_path = \"/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs\"  \nprint(\"Number of test files:\", len(os.listdir(test_path)))\nprint(\"Example files:\", os.listdir(test_path)[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:22:15.130266Z","iopub.execute_input":"2024-11-22T21:22:15.130573Z","iopub.status.idle":"2024-11-22T21:22:21.857625Z","shell.execute_reply.started":"2024-11-22T21:22:15.130547Z","shell.execute_reply":"2024-11-22T21:22:21.856809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/safespeak-baseline/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:22:24.718035Z","iopub.execute_input":"2024-11-22T21:22:24.718865Z","iopub.status.idle":"2024-11-22T21:22:24.722658Z","shell.execute_reply.started":"2024-11-22T21:22:24.718830Z","shell.execute_reply":"2024-11-22T21:22:24.721870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/safespeak-baseline/model\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:10:32.689884Z","iopub.execute_input":"2024-11-22T19:10:32.690244Z","iopub.status.idle":"2024-11-22T19:10:32.695892Z","shell.execute_reply.started":"2024-11-22T19:10:32.690213Z","shell.execute_reply":"2024-11-22T19:10:32.694849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/safespeak-baseline/\") \n\nfrom model.models import get_model \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:10:38.986138Z","iopub.execute_input":"2024-11-22T19:10:38.986895Z","iopub.status.idle":"2024-11-22T19:10:38.991225Z","shell.execute_reply.started":"2024-11-22T19:10:38.986856Z","shell.execute_reply":"2024-11-22T19:10:38.990286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    \"model\": \"Res2TCNGuard\",  # Название модели\n    \"num_class\": 2,\n    \"gpu_id\": 0,\n    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n    \"d_args\": {\n        \"nb_samp\": 64600,\n        \"first_conv\": 128,\n        \"filts\": [70, [1, 32], [32, 32], [32, 64], [64, 64]]\n    },\n    \"checkpoint\": \"/kaggle/input/safespeak-baseline/weights/best_1.495.pth\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:10:40.978174Z","iopub.execute_input":"2024-11-22T19:10:40.978548Z","iopub.status.idle":"2024-11-22T19:10:40.984042Z","shell.execute_reply.started":"2024-11-22T19:10:40.978516Z","shell.execute_reply":"2024-11-22T19:10:40.982966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_model(config)\nmodel.eval()  # Переводим модель в режим оценки\n\nprint(\"Model loaded and ready for inference!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:10:43.166356Z","iopub.execute_input":"2024-11-22T19:10:43.167129Z","iopub.status.idle":"2024-11-22T19:10:43.453286Z","shell.execute_reply.started":"2024-11-22T19:10:43.167092Z","shell.execute_reply":"2024-11-22T19:10:43.452543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport torch\n\ndef preprocess_audio(file_path, target_length=64600):\n    audio, sr = librosa.load(file_path, sr=None)  \n    if len(audio) > target_length:\n        audio = audio[:target_length]\n    else:\n        padding = target_length - len(audio)\n        audio = np.pad(audio, (0, padding), mode=\"constant\")\n    return torch.tensor(audio).float().unsqueeze(0)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:10:46.242172Z","iopub.execute_input":"2024-11-22T19:10:46.242500Z","iopub.status.idle":"2024-11-22T19:10:46.248551Z","shell.execute_reply.started":"2024-11-22T19:10:46.242471Z","shell.execute_reply":"2024-11-22T19:10:46.247367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef predict(model, audio_path, device=\"cuda:0\"):\n    audio_tensor = preprocess_audio(audio_path).to(device)\n    with torch.no_grad():\n        _, output = model(audio_tensor)\n        prob_bonafide = torch.softmax(output, dim=1)[0][1].item()  # Вероятность\n    return prob_bonafide\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:10:46.581977Z","iopub.execute_input":"2024-11-22T19:10:46.582750Z","iopub.status.idle":"2024-11-22T19:10:46.587944Z","shell.execute_reply.started":"2024-11-22T19:10:46.582715Z","shell.execute_reply":"2024-11-22T19:10:46.586922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:12:20.666078Z","iopub.execute_input":"2024-11-22T19:12:20.667091Z","iopub.status.idle":"2024-11-22T19:12:29.653833Z","shell.execute_reply.started":"2024-11-22T19:12:20.667050Z","shell.execute_reply":"2024-11-22T19:12:29.652747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\nimport csv\n\ntest_path = \"/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs\"\nsubmission = []\n\nfor file_name in tqdm(os.listdir(test_path), desc=\"Processing files\"):\n    file_path = os.path.join(test_path, file_name)\n    prob_bonafide = predict(model, file_path, config[\"device\"])\n    submission.append((file_name.split('.')[0], prob_bonafide))\n\n\nsubmission_path = \"/kaggle/working/submission.csv\"\n\nwith open(submission_path, \"w\", newline=\"\") as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow([\"ID\", \"TARGET\"]) \n    writer.writerows(submission)\n\nprint(f\"Submission file saved to {submission_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:14:40.654586Z","iopub.execute_input":"2024-11-22T19:14:40.655635Z","iopub.status.idle":"2024-11-22T20:48:49.518357Z","shell.execute_reply.started":"2024-11-22T19:14:40.655587Z","shell.execute_reply":"2024-11-22T20:48:49.517362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RESNET (НОВЫЙ БЕЙЗЛАЙН)","metadata":{}},{"cell_type":"code","source":"import librosa\nimport numpy as np\nimport torch\n\ndef audio_to_melspectrogram(file_path, n_mels=128, n_fft=2048, hop_length=512):\n    \"\"\"\n    Преобразует аудиофайл в Mel-Spectrogram.\n    \"\"\"\n    audio, sr = librosa.load(file_path, sr=None)\n    mel_spec = librosa.feature.melspectrogram(audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:41:38.914372Z","iopub.execute_input":"2024-12-09T08:41:38.914748Z","iopub.status.idle":"2024-12-09T08:41:38.920626Z","shell.execute_reply.started":"2024-12-09T08:41:38.914716Z","shell.execute_reply":"2024-12-09T08:41:38.919694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.models as models\nfrom torch import nn\n\nclass AudioResNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(AudioResNet, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)  # Загружаем предобученную ResNet18\n        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)  # Меняем последний слой\n\n    def forward(self, x):\n        return self.resnet(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:41:39.303677Z","iopub.execute_input":"2024-12-09T08:41:39.304348Z","iopub.status.idle":"2024-12-09T08:41:39.310181Z","shell.execute_reply.started":"2024-12-09T08:41:39.304319Z","shell.execute_reply":"2024-12-09T08:41:39.309171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = AudioResNet().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:40:23.742836Z","iopub.execute_input":"2024-12-09T08:40:23.743478Z","iopub.status.idle":"2024-12-09T08:40:24.659588Z","shell.execute_reply.started":"2024-12-09T08:40:23.743448Z","shell.execute_reply":"2024-12-09T08:40:24.658798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10, save_path=\"model_weights.pth\"):\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        \n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f}\")\n        \n        # Оценка на валидации\n        validate_model(model, val_loader, criterion, device)\n    \n    # Сохранение весов модели после всех эпох\n    torch.save(model.state_dict(), save_path)\n    print(f\"Model weights saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T09:31:50.465676Z","iopub.execute_input":"2024-11-23T09:31:50.466052Z","iopub.status.idle":"2024-11-23T09:31:50.472526Z","shell.execute_reply.started":"2024-11-23T09:31:50.466019Z","shell.execute_reply":"2024-11-23T09:31:50.471658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    :param y_true: Истинные метки (0 или 1)\n    :param y_pred: Предсказанные вероятности для класса 1 (bonafide)\n    \"\"\"\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:40:30.080970Z","iopub.execute_input":"2024-12-09T08:40:30.081723Z","iopub.status.idle":"2024-12-09T08:40:30.086704Z","shell.execute_reply.started":"2024-12-09T08:40:30.081692Z","shell.execute_reply":"2024-12-09T08:40:30.085716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_model(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = 0\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            # Сохраняем предсказания и метки\n            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Вероятности для bonafide\n            all_preds.extend(probs)\n            all_labels.extend(labels.cpu().numpy())\n\n    # Вычисляем EER\n    eer, eer_threshold = calculate_eer(all_labels, all_preds)\n    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, EER: {eer:.4f}\")\n    return eer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:40:31.573835Z","iopub.execute_input":"2024-12-09T08:40:31.574481Z","iopub.status.idle":"2024-12-09T08:40:31.582067Z","shell.execute_reply.started":"2024-12-09T08:40:31.574432Z","shell.execute_reply":"2024-12-09T08:40:31.580945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport torch\nimport numpy as np\nimport librosa\n\nclass AudioDataset(Dataset):\n    def __init__(self, file_paths, labels, target_length=64600, n_mels=128, n_fft=2048, hop_length=512):\n        \"\"\"\n        file_paths: список путей к аудиофайлам\n        labels: список меток (0 для spoof, 1 для bonafide)\n        \"\"\"\n        self.file_paths = file_paths\n        self.labels = labels\n        self.target_length = target_length\n        self.n_mels = n_mels\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        label = self.labels[idx]\n\n        # Преобразуем аудио в Mel-Spectrogram\n        mel_spec = self._preprocess_audio(file_path)\n        \n        return mel_spec, torch.tensor(label, dtype=torch.long)\n\n    def _preprocess_audio(self, file_path):\n        \"\"\"\n        Преобразует аудиофайл в Mel-Spectrogram\n        \"\"\"\n        audio, sr = librosa.load(file_path, sr=None)\n        \n        # Приведение к фиксированной длине\n        if len(audio) > self.target_length:\n            audio = audio[:self.target_length]\n        else:\n            padding = self.target_length - len(audio)\n            audio = np.pad(audio, (0, padding), mode=\"constant\")\n        \n        # Создание Mel-Spectrogram\n        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=self.n_fft,\n                                                  hop_length=self.hop_length, n_mels=self.n_mels)\n        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        mel_spec = torch.tensor(mel_spec, dtype=torch.float).unsqueeze(0)  # Добавляем канал\n        return mel_spec\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:41:52.820062Z","iopub.execute_input":"2024-12-09T08:41:52.820415Z","iopub.status.idle":"2024-12-09T08:41:52.829190Z","shell.execute_reply.started":"2024-12-09T08:41:52.820385Z","shell.execute_reply":"2024-12-09T08:41:52.828353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Пути \ntrain_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\ntrain_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n\nval_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"\nval_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n\n\ndef load_data(data_path, protocol_path):\n    file_paths = []\n    labels = []\n    \n    with open(protocol_path, \"r\") as f:\n        for line in f:\n            columns = line.strip().split()\n            utt_id = columns[1]  # Используем вторую колонку\n            label = columns[-1]  # Последняя колонка содержит метку ('bonafide' или 'spoof')\n\n            file_paths.append(os.path.join(data_path, f\"{utt_id}.flac\"))\n            labels.append(1 if label == \"bonafide\" else 0)\n\n    return file_paths, labels\n\n\ntrain_files, train_labels = load_data(train_data_path, train_protocol_path)\n\nval_files, val_labels = load_data(val_data_path, val_protocol_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T09:32:01.987846Z","iopub.execute_input":"2024-11-23T09:32:01.988166Z","iopub.status.idle":"2024-11-23T09:32:02.090094Z","shell.execute_reply.started":"2024-11-23T09:32:01.988139Z","shell.execute_reply":"2024-11-23T09:32:02.089211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\ntrain_dataset = AudioDataset(train_files, train_labels)\nval_dataset = AudioDataset(val_files, val_labels)\n\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\nprint(f\"Train size: {len(train_loader.dataset)}, Validation size: {len(val_loader.dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T09:32:03.050553Z","iopub.execute_input":"2024-11-23T09:32:03.051430Z","iopub.status.idle":"2024-11-23T09:32:03.057856Z","shell.execute_reply.started":"2024-11-23T09:32:03.051386Z","shell.execute_reply":"2024-11-23T09:32:03.056775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ОБУЧЕНИЕ","metadata":{}},{"cell_type":"code","source":"model = AudioResNet().to(device)\n\ntrain_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10, save_path=\"RESNET_weights.pth\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T09:33:33.778698Z","iopub.execute_input":"2024-11-23T09:33:33.779075Z","iopub.status.idle":"2024-11-23T12:20:05.701291Z","shell.execute_reply.started":"2024-11-23T09:33:33.779043Z","shell.execute_reply":"2024-11-23T12:20:05.700299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_competition(model, test_path, device):\n    \"\"\"\n    Выполняет предсказания для файлов в тестовой папке и сохраняет результаты в файл.\n    \"\"\"\n    import csv\n    from tqdm import tqdm\n\n    model.eval()\n    submission = []\n\n    for file_name in tqdm(os.listdir(test_path), desc=\"Predicting competition data\"):\n        file_path = os.path.join(test_path, file_name)\n        \n        # Используем функцию preprocess_audio\n        mel_spec = preprocess_audio(file_path).to(device)\n        \n        with torch.no_grad():\n            output = model(mel_spec.unsqueeze(0))  # Добавляем размер батча\n            prob = torch.softmax(output, dim=1)[0][1].item()  # Вероятность bonafide\n\n        submission.append((file_name.split('.')[0], prob))\n\n    # Сохраняем предсказания в файл\n    submission_path = \"/kaggle/working/submission_resnet.csv\"\n    with open(submission_path, \"w\", newline=\"\") as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"ID\", \"score\"])\n        writer.writerows(submission)\n\n    print(f\"Submission file saved to {submission_path}\")\n    return submission_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T12:31:00.318165Z","iopub.execute_input":"2024-11-23T12:31:00.318959Z","iopub.status.idle":"2024-11-23T12:31:00.325193Z","shell.execute_reply.started":"2024-11-23T12:31:00.318926Z","shell.execute_reply":"2024-11-23T12:31:00.324326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_audio(file_path, target_length=64600, n_mels=128, n_fft=2048, hop_length=512):\n    \"\"\"\n    Преобразует аудиофайл в Mel-Spectrogram фиксированного размера.\n    \"\"\"\n    import librosa\n    import numpy as np\n    import torch\n\n    audio, sr = librosa.load(file_path, sr=None)\n    \n    # Приведение к фиксированной длине\n    if len(audio) > target_length:\n        audio = audio[:target_length]\n    else:\n        padding = target_length - len(audio)\n        audio = np.pad(audio, (0, padding), mode=\"constant\")\n    \n    # Создание Mel-Spectrogram\n    mel_spec = librosa.feature.melspectrogram(\n        y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n    )\n    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n    mel_spec = torch.tensor(mel_spec, dtype=torch.float).unsqueeze(0)  # Добавляем канал\n    return mel_spec\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:42:19.247010Z","iopub.execute_input":"2024-12-09T08:42:19.247730Z","iopub.status.idle":"2024-12-09T08:42:19.254137Z","shell.execute_reply.started":"2024-12-09T08:42:19.247697Z","shell.execute_reply":"2024-12-09T08:42:19.253067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_path = \"/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs\"\n\nsubmission_file = predict_competition(model, test_path, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T12:31:02.286333Z","iopub.execute_input":"2024-11-23T12:31:02.286600Z","iopub.status.idle":"2024-11-23T13:42:16.881539Z","shell.execute_reply.started":"2024-11-23T12:31:02.286574Z","shell.execute_reply":"2024-11-23T13:42:16.880723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/RESNET_weights.pth\"\n\n\ndisplay(FileLink(\"RESNET_weights.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T13:56:56.210103Z","iopub.execute_input":"2024-11-23T13:56:56.210433Z","iopub.status.idle":"2024-11-23T13:56:56.216272Z","shell.execute_reply.started":"2024-11-23T13:56:56.210404Z","shell.execute_reply":"2024-11-23T13:56:56.215439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ВАЛИДАЦИЯ","metadata":{}},{"cell_type":"code","source":"# Импортируем ResNet и создаем экземпляр\naudio_resnet = AudioResNet(num_classes=2).to(device)\n\n# Загрузка сохраненных весов\ncheckpoint_path = \"/kaggle/input/resnet_v1/pytorch/default/1/RESNET_weights.pth\"  # Укажите путь к весам\nif os.path.exists(checkpoint_path):\n    audio_resnet.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    print(\"ResNet weights loaded successfully.\")\nelse:\n    print(\"ResNet weights file not found. Model will start training from scratch.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:42:26.757283Z","iopub.execute_input":"2024-12-09T08:42:26.757970Z","iopub.status.idle":"2024-12-09T08:42:27.033297Z","shell.execute_reply.started":"2024-12-09T08:42:26.757936Z","shell.execute_reply":"2024-12-09T08:42:27.032318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, f1_score, accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\n\naudio_resnet.eval()\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n# Проверка на валидации\nall_labels = []\nall_scores = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n        real_data = real_data.to(device)  \n        labels = labels.to(device)\n\n        # Прогон через ResNet\n        real_output = audio_resnet(real_data)\n        probabilities = torch.softmax(real_output, dim=1)[:, 1]  # Вероятности класса \"подлинный\"\n        all_scores.extend(probabilities.cpu().numpy().flatten())  # Сохраняем вероятности\n        all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n\n# Преобразуем вероятности в предсказанные классы\nthreshold = 0.5  # Порог по умолчанию\npredicted_classes = np.array(all_scores) >= threshold\n\n# Расчёт метрик\neer, eer_threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\nf1 = f1_score(all_labels, predicted_classes)\naccuracy = accuracy_score(all_labels, predicted_classes)\n\n# Вывод результатов\nprint(f\"Validation Metrics:\")\nprint(f\"  EER: {eer:.4f}, Threshold: {eer_threshold:.4f}\")\nprint(f\"  F1 Score: {f1:.4f}\")\nprint(f\"  Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:42:31.174022Z","iopub.execute_input":"2024-12-09T08:42:31.174395Z","iopub.status.idle":"2024-12-09T08:42:31.366941Z","shell.execute_reply.started":"2024-12-09T08:42:31.174360Z","shell.execute_reply":"2024-12-09T08:42:31.365355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**РЕЗУЛЬТАТЫ ВАЛИДАЦИИ**\n\nValidation Metrics:\r\n  EER: 0.4600, Threshold: 0.3250\r\n  F1 Score: 0.2546\r\n  Accuracy: 0.5186","metadata":{}},{"cell_type":"markdown","source":"# Реализация WaveGAN","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nimport csv \nfrom tqdm import tqdm\nimport os\nimport torch\nimport librosa\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:31:52.828924Z","iopub.execute_input":"2024-12-12T11:31:52.829694Z","iopub.status.idle":"2024-12-12T11:31:52.834679Z","shell.execute_reply.started":"2024-12-12T11:31:52.829658Z","shell.execute_reply":"2024-12-12T11:31:52.833780Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:31:53.217726Z","iopub.execute_input":"2024-12-12T11:31:53.218503Z","iopub.status.idle":"2024-12-12T11:31:53.276701Z","shell.execute_reply.started":"2024-12-12T11:31:53.218467Z","shell.execute_reply":"2024-12-12T11:31:53.275672Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, file_paths, labels, target_length=64600):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.target_length = target_length\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        label = self.labels[idx]\n\n        # Загружаем и обрабатываем аудио\n        audio, sr = librosa.load(file_path, sr=None)\n        if len(audio) > self.target_length:\n            audio = audio[:self.target_length]\n        else:\n            padding = self.target_length - len(audio)\n            audio = np.pad(audio, (0, padding), mode=\"constant\")\n\n        # Преобразуем в Tensor и добавляем канал\n        audio = torch.tensor(audio, dtype=torch.float).unsqueeze(0)  # [1, T]\n        return audio, torch.tensor(label, dtype=torch.long)\n\n\n    def _preprocess_audio(self, file_path):\n        \"\"\"\n        Загружает аудио, приводит его к фиксированной длине и возвращает тензор [1, T].\n        \"\"\"\n        # Загрузка аудио\n        audio, sr = librosa.load(file_path, sr=None)\n\n        # Приведение к фиксированной длине\n        if len(audio) > self.target_length:\n            audio = audio[:self.target_length]\n        else:\n            padding = self.target_length - len(audio)\n            audio = np.pad(audio, (0, padding), mode=\"constant\")\n\n        # Преобразование в Tensor с добавлением измерения канала\n        audio = torch.tensor(audio, dtype=torch.float).unsqueeze(0)  # [1, T]\n        return audio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:31:53.752715Z","iopub.execute_input":"2024-12-12T11:31:53.753392Z","iopub.status.idle":"2024-12-12T11:31:53.761330Z","shell.execute_reply.started":"2024-12-12T11:31:53.753359Z","shell.execute_reply":"2024-12-12T11:31:53.760646Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Загрузка файлов и меток","metadata":{}},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n# Пути к данным\ntrain_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\nval_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"  # Путь к валидационной папке\ntrain_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\nval_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n\n# Тренировочные данные\ntrain_file_paths = []\ntrain_labels = []\n\nwith open(train_protocol_path, \"r\") as f:\n    for line in f:\n        columns = line.strip().split()\n        utt_id = columns[1]\n        label = columns[-1]\n        train_file_paths.append(os.path.join(train_data_path, f\"{utt_id}.flac\"))\n        train_labels.append(1 if label == \"bonafide\" else 0)\n\n# Перемешиваем тренировочные данные\ntrain_file_paths, train_labels = shuffle(train_file_paths, train_labels, random_state=42)\n\n\n# Валидационные данные (фиксированное количество записей)\nval_file_paths = []\nval_labels = []\n\nwith open(val_protocol_path, \"r\") as f:\n    for i, line in enumerate(f):\n        columns = line.strip().split()\n        utt_id = columns[1]\n        label = columns[-1]\n        val_file_paths.append(os.path.join(val_data_path, f\"{utt_id}.flac\"))\n        val_labels.append(1 if label == \"bonafide\" else 0)\n\n\n\n# Перемешиваем валидационные данные\nval_file_paths, val_labels = shuffle(val_file_paths, val_labels, random_state=42)\n\n# # Ограничиваем до 1000 записей\n# val_file_paths = val_file_paths[:2000]\n# val_labels = val_labels[:2000]\n\n# Проверяем размеры\nprint(f\"Train set size: {len(train_file_paths)} files\")\nprint(f\"Validation set size: {len(val_file_paths)} files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:50:40.840148Z","iopub.execute_input":"2024-12-12T12:50:40.840969Z","iopub.status.idle":"2024-12-12T12:50:40.963849Z","shell.execute_reply.started":"2024-12-12T12:50:40.840933Z","shell.execute_reply":"2024-12-12T12:50:40.963062Z"}},"outputs":[{"name":"stdout","text":"Train set size: 25380 files\nValidation set size: 24844 files\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**ВЕРСИЯ данных 2!**","metadata":{}},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n# Пути к данным\ntrain_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\nval_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"\ntrain_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\nval_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n\n# Функция загрузки данных\ndef load_data(protocol_path, data_path, filter_bonafide=False, limit=None):\n    file_paths = []\n    labels = []\n    with open(protocol_path, \"r\") as f:\n        for line in f:\n            columns = line.strip().split()\n            utt_id = columns[1]\n            label = 1 if columns[-1] == \"bonafide\" else 0\n            if filter_bonafide and label == 0:\n                continue  # Пропускаем записи с меткой spoof\n            file_paths.append(os.path.join(data_path, f\"{utt_id}.flac\"))\n            labels.append(label)\n    if limit:\n        file_paths, labels = shuffle(file_paths, labels, random_state=42)\n        file_paths = file_paths[:limit]\n        labels = labels[:limit]\n    return file_paths, labels\n\n# Загружаем данные\ntrain_file_paths, train_labels = load_data(\n    train_protocol_path, train_data_path, filter_bonafide=True\n)\nval_file_paths, val_labels = load_data(\n    val_protocol_path, val_data_path, filter_bonafide=False, limit=1500\n)\n\n# Проверяем размеры\nprint(f\"Train set size (bonafide only): {len(train_file_paths)} files\")\nprint(f\"Validation set size: {len(val_file_paths)} files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:25:20.507133Z","iopub.execute_input":"2024-12-02T17:25:20.508115Z","iopub.status.idle":"2024-12-02T17:25:20.574922Z","shell.execute_reply.started":"2024-12-02T17:25:20.508077Z","shell.execute_reply":"2024-12-02T17:25:20.574165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ВЕРСИЯ данных 3!**","metadata":{}},{"cell_type":"code","source":"import os\nfrom sklearn.utils import shuffle\n\n# Пути к данным\ntrain_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\nval_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"\ntrain_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\nval_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n\ndef load_and_balance_data(protocol_path, data_path):\n    file_paths = []\n    labels = []\n\n    # Считываем данные\n    with open(protocol_path, \"r\") as f:\n        for line in f:\n            columns = line.strip().split()\n            utt_id = columns[1]\n            label = columns[-1]\n            file_paths.append(os.path.join(data_path, f\"{utt_id}.flac\"))\n            labels.append(1 if label == \"bonafide\" else 0)\n\n    # Разделяем данные по классам\n    real_files = [(path, label) for path, label in zip(file_paths, labels) if label == 1]\n    fake_files = [(path, label) for path, label in zip(file_paths, labels) if label == 0]\n\n    # Вычисляем количество записей меньшего класса\n    min_class_size = min(len(real_files), len(fake_files))\n\n    # Балансируем данные\n    real_files = real_files[:min_class_size]\n    fake_files = fake_files[:min_class_size]\n\n    balanced_data = real_files + fake_files\n    # Перемешиваем данные\n    balanced_data = shuffle(balanced_data, random_state=42)\n\n    # Разделяем пути и метки\n    balanced_file_paths, balanced_labels = zip(*balanced_data)\n    return list(balanced_file_paths), list(balanced_labels)\n\n# Тренировочные данные\ntrain_file_paths, train_labels = load_and_balance_data(train_protocol_path, train_data_path)\n\n# Валидационные данные\nval_file_paths, val_labels = load_and_balance_data(val_protocol_path, val_data_path)\n\n# Проверяем размеры наборов данных\nprint(f\"Train set size: {len(train_file_paths)} files, Real: {train_labels.count(1)}, Fake: {train_labels.count(0)}\")\nprint(f\"Validation set size: {len(val_file_paths)} files, Real: {val_labels.count(1)}, Fake: {val_labels.count(0)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:50:16.076655Z","iopub.execute_input":"2024-12-09T08:50:16.077019Z","iopub.status.idle":"2024-12-09T08:50:16.203020Z","shell.execute_reply.started":"2024-12-09T08:50:16.076991Z","shell.execute_reply":"2024-12-09T08:50:16.202152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom sklearn.utils import shuffle\n\n# Пути к данным\ntrain_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\nval_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"\ntrain_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\nval_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n\ndef load_and_balance_data(protocol_path, data_path):\n    file_paths = []\n    labels = []\n\n    # Считываем данные\n    with open(protocol_path, \"r\") as f:\n        for line in f:\n            columns = line.strip().split()\n            utt_id = columns[1]\n            label = columns[-1]\n            file_paths.append(os.path.join(data_path, f\"{utt_id}.flac\"))\n            labels.append(1 if label == \"bonafide\" else 0)\n\n    # Разделяем данные по классам\n    real_files = [(path, label) for path, label in zip(file_paths, labels) if label == 1]\n    fake_files = [(path, label) for path, label in zip(file_paths, labels) if label == 0]\n\n    # Вычисляем количество записей меньшего класса\n    min_class_size = min(len(real_files), len(fake_files))\n\n    # Балансируем данные\n    real_files = real_files[:min_class_size]\n    fake_files = fake_files[:min_class_size]\n\n    balanced_data = real_files + fake_files\n    # Перемешиваем данные\n    balanced_data = shuffle(balanced_data, random_state=42)\n\n    # Разделяем пути и метки\n    balanced_file_paths, balanced_labels = zip(*balanced_data)\n    return list(balanced_file_paths), list(balanced_labels)\n\n\n# Валидационные данные\nval_file_paths, val_labels = load_and_balance_data(val_protocol_path, val_data_path)\n\n\n# Проверяем размеры наборов данных\n\nprint(f\"Validation set size: {len(val_file_paths)} files, Real: {val_labels.count(1)}, Fake: {val_labels.count(0)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T13:51:40.140533Z","iopub.execute_input":"2024-12-04T13:51:40.140921Z","iopub.status.idle":"2024-12-04T13:51:40.199339Z","shell.execute_reply.started":"2024-12-04T13:51:40.140888Z","shell.execute_reply":"2024-12-04T13:51:40.198648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#ВЕРСИЯ 4\nimport os\nfrom sklearn.utils import shuffle\n\n# Пути к данным\ntrain_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\ndev_data_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"  # Путь к dev папке\ntrain_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\ndev_protocol_path = \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n\n# Тренировочные данные\ntrain_file_paths = []\ntrain_labels = []\n\n# Загрузка тренировочного набора\nwith open(train_protocol_path, \"r\") as f:\n    for line in f:\n        columns = line.strip().split()\n        utt_id = columns[1]\n        label = columns[-1]\n        train_file_paths.append(os.path.join(train_data_path, f\"{utt_id}.flac\"))\n        train_labels.append(1 if label == \"bonafide\" else 0)\n\n# Загрузка dev набора и добавление к тренировочному\nwith open(dev_protocol_path, \"r\") as f:\n    for line in f:\n        columns = line.strip().split()\n        utt_id = columns[1]\n        label = columns[-1]\n        train_file_paths.append(os.path.join(dev_data_path, f\"{utt_id}.flac\"))\n        train_labels.append(1 if label == \"bonafide\" else 0)\n\n# Перемешиваем объединенные тренировочные данные\ntrain_file_paths, train_labels = shuffle(train_file_paths, train_labels, random_state=42)\n\n# Проверяем размеры\nprint(f\"Combined train set size: {len(train_file_paths)} files\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:32:01.343855Z","iopub.execute_input":"2024-12-12T11:32:01.344209Z","iopub.status.idle":"2024-12-12T11:32:01.471543Z","shell.execute_reply.started":"2024-12-12T11:32:01.344177Z","shell.execute_reply":"2024-12-12T11:32:01.470663Z"}},"outputs":[{"name":"stdout","text":"Combined train set size: 50224 files\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"np.unique(val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:32:02.111228Z","iopub.execute_input":"2024-12-12T11:32:02.111606Z","iopub.status.idle":"2024-12-12T11:32:02.119398Z","shell.execute_reply.started":"2024-12-12T11:32:02.111574Z","shell.execute_reply":"2024-12-12T11:32:02.118431Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([0, 1])"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### Даталоадеры","metadata":{}},{"cell_type":"code","source":"# Создаем датасеты и лоадеры\nbatch_size = 16\n\ntrain_dataset = AudioDataset(train_file_paths, train_labels)\nval_dataset = AudioDataset(val_file_paths, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:53:42.346721Z","iopub.execute_input":"2024-12-12T12:53:42.347278Z","iopub.status.idle":"2024-12-12T12:53:42.354926Z","shell.execute_reply.started":"2024-12-12T12:53:42.347242Z","shell.execute_reply":"2024-12-12T12:53:42.354025Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Визуализация","metadata":{}},{"cell_type":"code","source":"# def visualize_spectrogram(mel_spec, title=\"Mel-Spectrogram\"):\n#     mel_spec = mel_spec.squeeze(0).numpy()  # Убираем лишний размер, получаем [n_mels, T]\n#     plt.figure(figsize=(10, 4))\n#     librosa.display.specshow(mel_spec, sr=16000, hop_length=512,\n#                              x_axis='time', y_axis='mel', cmap='viridis')\n#     plt.colorbar(format='%+2.0f dB')\n#     plt.title(title)\n#     plt.tight_layout()\n#     plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:49:14.070987Z","iopub.execute_input":"2024-12-09T17:49:14.072128Z","iopub.status.idle":"2024-12-09T17:49:14.076391Z","shell.execute_reply.started":"2024-12-09T17:49:14.072085Z","shell.execute_reply":"2024-12-09T17:49:14.075472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for i in range(2):\n#     mel_spec, label = train_dataset[i]\n#     title = \"Bonafide\" if label == 1 else \"Spoof\"\n#     visualize_spectrogram(mel_spec, title=title)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:49:14.332511Z","iopub.execute_input":"2024-12-09T17:49:14.333217Z","iopub.status.idle":"2024-12-09T17:49:14.337090Z","shell.execute_reply.started":"2024-12-09T17:49:14.333183Z","shell.execute_reply":"2024-12-09T17:49:14.336005Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Генератор","metadata":{}},{"cell_type":"code","source":"class WaveGANGenerator(nn.Module):\n    def __init__(self, latent_dim=100, output_length=64600):\n        super(WaveGANGenerator, self).__init__()\n        self.latent_dim = latent_dim\n        self.output_length = output_length\n\n        self.fc = nn.Linear(latent_dim, 256 * 200)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.relu = nn.ReLU()\n\n        self.model = nn.Sequential(\n            nn.Upsample(size=800),\n            nn.Conv1d(256, 128, kernel_size=25, stride=1, padding=12),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n\n            nn.Upsample(size=3200),\n            nn.Conv1d(128, 64, kernel_size=25, stride=1, padding=12),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n\n            nn.Upsample(size=12800),\n            nn.Conv1d(64, 32, kernel_size=25, stride=1, padding=12),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n\n            nn.Upsample(size=64600),\n            nn.Conv1d(32, 1, kernel_size=25, stride=1, padding=12),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        x = self.fc(z)\n        x = x.view(z.size(0), 256, -1)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.model(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:32:10.460029Z","iopub.execute_input":"2024-12-12T11:32:10.460377Z","iopub.status.idle":"2024-12-12T11:32:10.470909Z","shell.execute_reply.started":"2024-12-12T11:32:10.460345Z","shell.execute_reply":"2024-12-12T11:32:10.470013Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Дискриминатор","metadata":{}},{"cell_type":"code","source":"class WaveGANDiscriminator(nn.Module):\n    def __init__(self, input_length=64600):  # Длина входного 1D аудиосигнала\n        super(WaveGANDiscriminator, self).__init__()\n        self.input_length = input_length\n\n        self.model = nn.Sequential(\n            nn.Conv1d(1, 64, kernel_size=25, stride=4, padding=11),  # Уменьшаем размерность\n            nn.LeakyReLU(0.2),\n\n            nn.Conv1d(64, 128, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv1d(128, 256, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv1d(256, 512, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.Flatten(),  # Преобразуем в вектор\n            nn.Linear(512 * (input_length // (4 ** 4)), 1),  # Финальный полносвязный слой\n            nn.Sigmoid()  # Вероятность [0, 1]\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:56:58.960981Z","iopub.execute_input":"2024-12-05T07:56:58.961861Z","iopub.status.idle":"2024-12-05T07:56:58.968995Z","shell.execute_reply.started":"2024-12-05T07:56:58.961828Z","shell.execute_reply":"2024-12-05T07:56:58.967821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#ВЕРСИЯ 2!!!\nclass WaveGANDiscriminator(nn.Module):\n    def __init__(self, input_length=64600):\n        super(WaveGANDiscriminator, self).__init__()\n        self.input_length = input_length\n        self.model = nn.Sequential(\n            nn.Conv1d(1, 64, kernel_size=25, stride=4, padding=11),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv1d(64, 128, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv1d(128, 256, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv1d(256, 512, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.Flatten(),\n        )\n        self.fc_input_dim = self._calculate_fc_input_dim()\n        self.fc = nn.Sequential(\n            nn.Linear(self.fc_input_dim, 1),\n            nn.Sigmoid()\n        )\n\n    def _calculate_fc_input_dim(self):\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 1, self.input_length)  # Входной тензор\n            output = self.model(dummy_input)\n            return output.numel()\n\n    def forward(self, x):\n        x = self.model(x)\n        return self.fc(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:01:43.415018Z","iopub.execute_input":"2024-12-05T08:01:43.415971Z","iopub.status.idle":"2024-12-05T08:01:43.423497Z","shell.execute_reply.started":"2024-12-05T08:01:43.415930Z","shell.execute_reply":"2024-12-05T08:01:43.422735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#ВЕРСИЯ 3!!!\nimport torch\nfrom torch import nn\n\nclass WaveGANDiscriminator(nn.Module):\n    def __init__(self, input_length=64600, dropout_rate=0.3):  # Добавляем параметр dropout_rate\n        super(WaveGANDiscriminator, self).__init__()\n        self.input_length = input_length\n        self.model = nn.Sequential(\n            nn.Conv1d(1, 64, kernel_size=25, stride=4, padding=11),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(dropout_rate),  # Dropout после активации\n\n            nn.Conv1d(64, 128, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(128),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(dropout_rate),  # Dropout после активации\n\n            nn.Conv1d(128, 256, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(dropout_rate),  # Dropout после активации\n\n            nn.Conv1d(256, 512, kernel_size=25, stride=4, padding=11),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(dropout_rate),  # Dropout после активации\n\n            nn.Flatten(),\n        )\n        self.fc_input_dim = self._calculate_fc_input_dim()\n        self.fc = nn.Sequential(\n            nn.Linear(self.fc_input_dim, 1),\n            nn.Sigmoid()\n        )\n\n    def _calculate_fc_input_dim(self):\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 1, self.input_length)  # Входной тензор\n            output = self.model(dummy_input)\n            return output.numel()\n\n    def forward(self, x):\n        x = self.model(x)\n        return self.fc(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:32:14.771871Z","iopub.execute_input":"2024-12-12T11:32:14.772207Z","iopub.status.idle":"2024-12-12T11:32:14.781015Z","shell.execute_reply.started":"2024-12-12T11:32:14.772179Z","shell.execute_reply":"2024-12-12T11:32:14.779749Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Обучение WaveGAN","metadata":{}},{"cell_type":"code","source":"\ngenerator = WaveGANGenerator(latent_dim=100).to(device)  # Генератор\ndiscriminator = WaveGANDiscriminator().to(device)  # Дискриминатор\n\n# Оптимизаторы и функция потерь\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n\ncriterion = nn.BCELoss()  # Бинарная кросс-энтропия\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T09:50:51.894304Z","iopub.execute_input":"2024-12-03T09:50:51.894695Z","iopub.status.idle":"2024-12-03T09:50:52.209932Z","shell.execute_reply.started":"2024-12-03T09:50:51.894662Z","shell.execute_reply":"2024-12-03T09:50:52.208941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#VERSION 2\ngenerator = WaveGANGenerator(latent_dim=100).to(device)  # Генератор\ndiscriminator = WaveGANDiscriminator().to(device)\n\n\n# Оптимизаторы и функция потерь\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0005, betas=(0.5, 0.95))\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0005, betas=(0.5, 0.95))\n\ncriterion = nn.BCELoss()  # Бинарная кросс-энтропия\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:01:59.963320Z","iopub.execute_input":"2024-12-05T08:01:59.963664Z","iopub.status.idle":"2024-12-05T08:02:00.091831Z","shell.execute_reply.started":"2024-12-05T08:01:59.963636Z","shell.execute_reply":"2024-12-05T08:02:00.090881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#VERSION3\ngenerator = WaveGANGenerator(latent_dim=100).to(device)  # Генератор\ndiscriminator = WaveGANDiscriminator(input_length=64600, dropout_rate=0.45).to(device)\n\n\n# Оптимизаторы и функция потерь\ng_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.95))\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.95))\n\ncriterion = nn.BCELoss()  # Бинарная кросс-энтропия\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:32:29.753688Z","iopub.execute_input":"2024-12-12T11:32:29.754022Z","iopub.status.idle":"2024-12-12T11:32:31.141396Z","shell.execute_reply.started":"2024-12-12T11:32:29.753993Z","shell.execute_reply":"2024-12-12T11:32:31.140650Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"epochs = 15\nlatent_dim = 100\n\nz = torch.randn(batch_size, latent_dim).to(device)\nfake_data = generator(z)\nprint(f\"fake_data shape: {fake_data.shape}\")  # Должно быть [batch_size, 1, 64600]\n\nreal_data, _ = next(iter(train_loader))\nprint(f\"real_data shape: {real_data.shape}\")  # Должно быть [batch_size, 1, 64600]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T08:06:30.513044Z","iopub.execute_input":"2024-11-25T08:06:30.513922Z","iopub.status.idle":"2024-11-25T08:06:43.175733Z","shell.execute_reply.started":"2024-11-25T08:06:30.513885Z","shell.execute_reply":"2024-11-25T08:06:43.174758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 15\nlatent_dim = 100\n\nfor epoch in range(epochs):\n    for real_data, _ in train_loader:\n        real_data = real_data.to(device)\n        batch_size = real_data.size(0)\n\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        z = torch.randn(batch_size, latent_dim).to(device)\n        fake_data = generator(z)\n\n        real_output = discriminator(real_data)\n        fake_output = discriminator(fake_data.detach())\n\n        d_loss_real = criterion(real_output, real_labels)\n        d_loss_fake = criterion(fake_output, fake_labels)\n        d_loss = d_loss_real + d_loss_fake\n\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        fake_output = discriminator(fake_data)\n        g_loss = criterion(fake_output, real_labels)\n\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:09:35.289474Z","iopub.execute_input":"2024-11-23T20:09:35.289811Z","iopub.status.idle":"2024-11-23T22:24:03.605360Z","shell.execute_reply.started":"2024-11-23T20:09:35.289780Z","shell.execute_reply":"2024-11-23T22:24:03.604357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"/kaggle/working/gan_weights\"  # Путь для сохранения весов\nos.makedirs(save_path, exist_ok=True)  # Создаём директорию, если она не существует\n\n# Сохраняем веса\ntorch.save(generator.state_dict(), os.path.join(save_path, \"generator_final.pth\"))\ntorch.save(discriminator.state_dict(), os.path.join(save_path, \"discriminator_final.pth\"))\n\nprint(\"Model weights saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T08:17:30.255809Z","iopub.execute_input":"2024-11-24T08:17:30.256736Z","iopub.status.idle":"2024-11-24T08:17:30.336561Z","shell.execute_reply.started":"2024-11-24T08:17:30.256688Z","shell.execute_reply":"2024-11-24T08:17:30.335711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/gan_weights/generator_final.pth\"\n\n\ndisplay(FileLink(\"gan_weights/generator_final.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:26:56.120519Z","iopub.execute_input":"2024-11-23T22:26:56.120851Z","iopub.status.idle":"2024-11-23T22:26:56.126904Z","shell.execute_reply.started":"2024-11-23T22:26:56.120823Z","shell.execute_reply":"2024-11-23T22:26:56.125854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/gan_weights/discriminator_final.pth\"\n\n\ndisplay(FileLink(\"gan_weights/discriminator_final.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:27:16.215579Z","iopub.execute_input":"2024-11-23T22:27:16.215930Z","iopub.status.idle":"2024-11-23T22:27:16.221853Z","shell.execute_reply.started":"2024-11-23T22:27:16.215901Z","shell.execute_reply":"2024-11-23T22:27:16.220827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"/kaggle/input/gan-v1/pytorch/default/1\"  \n\n# Загрузка весов\ngenerator = WaveGANGenerator(latent_dim=100).to(device)\ndiscriminator = WaveGANDiscriminator().to(device)\n\ngenerator.load_state_dict(torch.load(os.path.join(save_path, \"generator_final.pth\")))\ndiscriminator.load_state_dict(torch.load(os.path.join(save_path, \"discriminator_final.pth\")))\n\ngenerator.eval()\ndiscriminator.eval()\n\nprint(\"Model weights loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T08:20:01.089111Z","iopub.execute_input":"2024-11-24T08:20:01.089837Z","iopub.status.idle":"2024-11-24T08:20:01.841224Z","shell.execute_reply.started":"2024-11-24T08:20:01.089804Z","shell.execute_reply":"2024-11-24T08:20:01.840352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n# Проверка на валидации\nall_labels = []\nall_scores = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n        real_data = real_data.to(device)  \n        labels = labels.to(device)\n\n        # Предсказания дискриминатора для реальных данных\n        real_output = discriminator(real_data)\n        all_scores.extend(real_output.cpu().numpy().flatten())  # Сохраняем вероятности\n        all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n\n# Расчёт EER\neer, threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\nprint(f\"Validation EER: {eer:.4f}, Threshold: {threshold:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T22:37:32.205415Z","iopub.execute_input":"2024-11-23T22:37:32.205842Z","iopub.status.idle":"2024-11-23T22:37:52.798907Z","shell.execute_reply.started":"2024-11-23T22:37:32.205808Z","shell.execute_reply":"2024-11-23T22:37:52.798153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv \nfrom tqdm import tqdm\nimport os\nimport torch\nimport librosa\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nimport numpy as np\n\ndef preprocess_audio(file_path, target_length=64600):\n    \"\"\"\n    Загружает аудио, приводит его к фиксированной длине и возвращает тензор [1, T].\n    \"\"\"\n    # Загрузка аудио\n    audio, sr = librosa.load(file_path, sr=None)\n\n    # Приведение к фиксированной длине\n    if len(audio) > target_length:\n        audio = audio[:target_length]\n    else:\n        padding = target_length - len(audio)\n        audio = np.pad(audio, (0, padding), mode=\"constant\")\n\n    # Преобразование в Tensor с добавлением измерения канала\n    audio = torch.tensor(audio, dtype=torch.float).unsqueeze(0)  # [1, T]\n    return audio\n\n\ndef predict_competition(discriminator, test_path, device, submission_path=\"/kaggle/working/submission_GAN.csv\"):\n\n    discriminator.eval()  # Переводим дискриминатор в режим оценки\n    submission = []\n\n    for file_name in tqdm(os.listdir(test_path), desc=\"Predicting competition data\"):\n        file_path = os.path.join(test_path, file_name)\n\n        # Предобработка аудио\n        audio_tensor = preprocess_audio(file_path).to(device)\n\n        with torch.no_grad():\n            # Прогон через дискриминатор\n            output = discriminator(audio_tensor.unsqueeze(1))  # Добавляем размер канала [1, T] -> [1, 1, T]\n            score = output.item()  # Получаем предсказанную вероятность\n\n        submission.append((file_name.split('.')[0], score))\n\n    # Сохраняем предсказания в файл\n    with open(submission_path, \"w\", newline=\"\") as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"ID\", \"score\"])\n        writer.writerows(submission)\n\n    print(f\"Submission file saved to {submission_path}\")\n    return submission_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T08:20:57.704041Z","iopub.execute_input":"2024-11-24T08:20:57.704617Z","iopub.status.idle":"2024-11-24T08:20:57.713209Z","shell.execute_reply.started":"2024-11-24T08:20:57.704582Z","shell.execute_reply":"2024-11-24T08:20:57.712328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_path = \"/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs\"\n\n# Выполняем предсказания\nsubmission_file = predict_competition(discriminator, test_path, device)\nprint(f\"Submission file created at: {submission_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T08:21:00.169839Z","iopub.execute_input":"2024-11-24T08:21:00.170495Z","iopub.status.idle":"2024-11-24T08:57:19.021963Z","shell.execute_reply.started":"2024-11-24T08:21:00.170461Z","shell.execute_reply":"2024-11-24T08:57:19.021084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ДООБУЧИМ НА РЕАЛЬНЫХ ЛЕЙБЛАХ И ПОДАВАЯ на вход генератора сгенерированные записи","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:28:11.062248Z","iopub.execute_input":"2024-11-27T17:28:11.063171Z","iopub.status.idle":"2024-11-27T17:28:11.067220Z","shell.execute_reply.started":"2024-11-27T17:28:11.063136Z","shell.execute_reply":"2024-11-27T17:28:11.066325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AudioEncoder(nn.Module):\n    def __init__(self, input_dim=64600, latent_dim=100):\n        super(AudioEncoder, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv1d(1, 128, kernel_size=15, stride=2, padding=7),\n            nn.ReLU(),\n            nn.Conv1d(128, 256, kernel_size=15, stride=2, padding=7),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(256 * (input_dim // 4), latent_dim)  # Преобразование в латентное пространство\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:17:27.390784Z","iopub.execute_input":"2024-11-24T09:17:27.391527Z","iopub.status.idle":"2024-11-24T09:17:27.397067Z","shell.execute_reply.started":"2024-11-24T09:17:27.391494Z","shell.execute_reply":"2024-11-24T09:17:27.396067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Оптимизаторы и функция потерь\nd_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\ng_optimizer = torch.optim.Adam(\n    list(generator.parameters()) + list(audio_encoder.parameters()), lr=0.0001, betas=(0.5, 0.999)\n)\n\ncriterion = nn.BCELoss()  # Бинарная кросс-энтропия\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:17:28.072710Z","iopub.execute_input":"2024-11-24T09:17:28.073392Z","iopub.status.idle":"2024-11-24T09:17:28.078881Z","shell.execute_reply.started":"2024-11-24T09:17:28.073360Z","shell.execute_reply":"2024-11-24T09:17:28.078084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 15\nlatent_dim = 100\naudio_encoder = AudioEncoder(input_dim=64600, latent_dim=latent_dim).to(device)\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n    for real_data, real_labels in tqdm(train_loader):\n        real_data = real_data.to(device)  # Аудио данные\n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Лейблы для дискриминатора\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n       \n\n        # Генерация фейковых данных\n        fake_data_input = real_data[real_labels.squeeze() == 0]  # Берем только заведомо фейковые данные\n        if fake_data_input.size(0) == 0:  # Проверка, если нет фейковых данных в этом батче\n            continue  # Пропускаем итерацию\n        fake_data_input = fake_data_input.to(device)\n        latent_vector = audio_encoder(fake_data_input)  # [batch_size, latent_dim]\n        fake_data = generator(latent_vector)  # Генерация данных с помощью генератора\n\n        # Подгоняем размеры лейблов к предсказаниям\n        current_batch_size = fake_data.size(0)\n        fake_labels = fake_labels[:current_batch_size]\n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data)  # Прогноз для реальных данных\n        fake_output = discriminator(fake_data.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels)\n        d_loss_fake = criterion(fake_output, fake_labels)\n        d_loss = d_loss_real + d_loss_fake\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Обучение генератора (улучшает фейковые данные)\n        fake_output = discriminator(fake_data)\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Цель: обмануть дискриминатор\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:43:59.690956Z","iopub.execute_input":"2024-11-24T09:43:59.691330Z","iopub.status.idle":"2024-11-24T11:44:23.970271Z","shell.execute_reply.started":"2024-11-24T09:43:59.691278Z","shell.execute_reply":"2024-11-24T11:44:23.969142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(generator.state_dict(), \"/kaggle/working/generator_checkpoint.pth\")\ntorch.save(discriminator.state_dict(), \"/kaggle/working/discriminator_checkpoint.pth\")\ntorch.save(audio_encoder.state_dict(), \"/kaggle/working/audio_encoder_checkpoint.pth\")\nprint(\"Checkpoint weights saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:44:30.383974Z","iopub.execute_input":"2024-11-24T11:44:30.384784Z","iopub.status.idle":"2024-11-24T11:44:33.808336Z","shell.execute_reply.started":"2024-11-24T11:44:30.384749Z","shell.execute_reply":"2024-11-24T11:44:33.807345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/generator_checkpoint.pth\"\n\n\ndisplay(FileLink(\"generator_checkpoint.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:45:16.754989Z","iopub.execute_input":"2024-11-24T11:45:16.755812Z","iopub.status.idle":"2024-11-24T11:45:16.761284Z","shell.execute_reply.started":"2024-11-24T11:45:16.755776Z","shell.execute_reply":"2024-11-24T11:45:16.760382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/discriminator_checkpoint.pth\"\n\n\ndisplay(FileLink(\"discriminator_checkpoint.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:45:24.195007Z","iopub.execute_input":"2024-11-24T11:45:24.195337Z","iopub.status.idle":"2024-11-24T11:45:24.201255Z","shell.execute_reply.started":"2024-11-24T11:45:24.195293Z","shell.execute_reply":"2024-11-24T11:45:24.200256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/audio_encoder_checkpoint.pth\"\n\n\ndisplay(FileLink(\"audio_encoder_checkpoint.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:45:30.637620Z","iopub.execute_input":"2024-11-24T11:45:30.637924Z","iopub.status.idle":"2024-11-24T11:45:30.643777Z","shell.execute_reply.started":"2024-11-24T11:45:30.637900Z","shell.execute_reply":"2024-11-24T11:45:30.643038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\n\n\ndiscriminator.eval()\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n# Проверка на валидации\nall_labels = []\nall_scores = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n        real_data = real_data.to(device)  \n        labels = labels.to(device)\n\n        # Предсказания дискриминатора для реальных данных\n        real_output = discriminator(real_data)\n        all_scores.extend(real_output.cpu().numpy().flatten())  # Сохраняем вероятности\n        all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n\n# Расчёт EER\neer, threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\nprint(f\"Validation EER: {eer:.4f}, Threshold: {threshold:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:46:45.227956Z","iopub.execute_input":"2024-11-24T11:46:45.228631Z","iopub.status.idle":"2024-11-24T11:47:08.373096Z","shell.execute_reply.started":"2024-11-24T11:46:45.228598Z","shell.execute_reply":"2024-11-24T11:47:08.372162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv \nfrom tqdm import tqdm\nimport os\nimport torch\nimport librosa\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nimport numpy as np\n\ndef preprocess_audio(file_path, target_length=64600):\n    \"\"\"\n    Загружает аудио, приводит его к фиксированной длине и возвращает тензор [1, T].\n    \"\"\"\n    # Загрузка аудио\n    audio, sr = librosa.load(file_path, sr=None)\n\n    # Приведение к фиксированной длине\n    if len(audio) > target_length:\n        audio = audio[:target_length]\n    else:\n        padding = target_length - len(audio)\n        audio = np.pad(audio, (0, padding), mode=\"constant\")\n\n    # Преобразование в Tensor с добавлением измерения канала\n    audio = torch.tensor(audio, dtype=torch.float).unsqueeze(0)  # [1, T]\n    return audio\n\n\ndef predict_competition(discriminator, test_path, device, submission_path=\"/kaggle/working/submission_GAN.csv\"):\n\n    discriminator.eval()  # Переводим дискриминатор в режим оценки\n    submission = []\n\n    for file_name in tqdm(os.listdir(test_path), desc=\"Predicting competition data\"):\n        file_path = os.path.join(test_path, file_name)\n\n        # Предобработка аудио\n        audio_tensor = preprocess_audio(file_path).to(device)\n\n        with torch.no_grad():\n            # Прогон через дискриминатор\n            output = discriminator(audio_tensor.unsqueeze(1))  # Добавляем размер канала [1, T] -> [1, 1, T]\n            score = output.item()  # Получаем предсказанную вероятность\n\n        submission.append((file_name.split('.')[0], score))\n\n    # Сохраняем предсказания в файл\n    with open(submission_path, \"w\", newline=\"\") as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"ID\", \"score\"])\n        writer.writerows(submission)\n\n    print(f\"Submission file saved to {submission_path}\")\n    return submission_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:48:57.109375Z","iopub.execute_input":"2024-11-24T11:48:57.109741Z","iopub.status.idle":"2024-11-24T11:48:57.118775Z","shell.execute_reply.started":"2024-11-24T11:48:57.109708Z","shell.execute_reply":"2024-11-24T11:48:57.117861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_path = \"/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs\"\n\n# Выполняем предсказания\nsubmission_file = predict_competition(discriminator, test_path, device)\nprint(f\"Submission file created at: {submission_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:49:06.095203Z","iopub.execute_input":"2024-11-24T11:49:06.095603Z","iopub.status.idle":"2024-11-24T12:26:37.938558Z","shell.execute_reply.started":"2024-11-24T11:49:06.095571Z","shell.execute_reply":"2024-11-24T12:26:37.937625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## СНОВА ПРОБУЕМ ПОДАВАТЬ ШУМ","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/gan-v1/pytorch/default/3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T08:13:07.593040Z","iopub.execute_input":"2024-11-25T08:13:07.593881Z","iopub.status.idle":"2024-11-25T08:13:08.622260Z","shell.execute_reply.started":"2024-11-25T08:13:07.593845Z","shell.execute_reply":"2024-11-25T08:13:08.621259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"/kaggle/input/gan-v1/pytorch/default/3\"  \n\n# Загрузка весов\ngenerator = WaveGANGenerator(latent_dim=100).to(device)\ndiscriminator = WaveGANDiscriminator().to(device)\n\ngenerator.load_state_dict(torch.load(os.path.join(save_path, \"generator_checkpoint.pth\")))\ndiscriminator.load_state_dict(torch.load(os.path.join(save_path, \"discriminator_checkpoint.pth\")))\n\n# generator.eval()\n# discriminator.eval()\n\nprint(\"Model weights loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T08:14:02.008097Z","iopub.execute_input":"2024-11-25T08:14:02.008460Z","iopub.status.idle":"2024-11-25T08:14:02.319525Z","shell.execute_reply.started":"2024-11-25T08:14:02.008429Z","shell.execute_reply":"2024-11-25T08:14:02.318650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 6\nlatent_dim = 100\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n\n    \n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n    \n    for real_data, real_labels in tqdm(train_loader):\n        real_data = real_data.to(device)  # Аудио данные\n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Лейблы для дискриминатора\n        fake_labels = torch.zeros(batch_size, 1).to(device)  # Лейблы для фейковых данных\n\n        # Генерация фейковых данных на основе шума\n        z = torch.randn(batch_size, latent_dim).to(device)  # Генератор работает с шумом\n        fake_data = generator(z)  # Генерация данных с помощью генератора\n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data)  # Прогноз для реальных данных\n        fake_output = discriminator(fake_data.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels)  # Используем реальные метки (0 и 1)\n        d_loss_fake = criterion(fake_output, fake_labels)  # Метки для фейковых данных - всегда 0\n        d_loss = d_loss_real + d_loss_fake\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Обучение генератора (улучшает фейковые данные)\n        fake_output = discriminator(fake_data)  # Генератор пытается обмануть дискриминатор\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Цель генератора - добиться, чтобы фейковые данные приняли за настоящие (метка 1)\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T08:25:35.530443Z","iopub.execute_input":"2024-11-25T08:25:35.530812Z","iopub.status.idle":"2024-11-25T09:36:26.071610Z","shell.execute_reply.started":"2024-11-25T08:25:35.530779Z","shell.execute_reply":"2024-11-25T09:36:26.070772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\n\n\ndiscriminator.eval()\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n# Проверка на валидации\nall_labels = []\nall_scores = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n        real_data = real_data.to(device)  \n        labels = labels.to(device)\n\n        # Предсказания дискриминатора для реальных данных\n        real_output = discriminator(real_data)\n        all_scores.extend(real_output.cpu().numpy().flatten())  # Сохраняем вероятности\n        all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n\n# Расчёт EER\neer, threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\nprint(f\"Validation EER: {eer:.4f}, Threshold: {threshold:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:36:31.980317Z","iopub.execute_input":"2024-11-25T09:36:31.981034Z","iopub.status.idle":"2024-11-25T09:36:50.599393Z","shell.execute_reply.started":"2024-11-25T09:36:31.981001Z","shell.execute_reply":"2024-11-25T09:36:50.598486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ПРОБУЕМ ПОДАВАТЬ ТОЛЬКО Истинные записи from train","metadata":{}},{"cell_type":"code","source":"epochs = 6\nlatent_dim = 100\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n\n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n    \n    for real_data, real_labels in tqdm(train_loader):\n        # Перевод данных на устройство\n        real_data = real_data.to(device)\n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Генерация фейковых данных на основе шума\n        z = torch.randn(batch_size, latent_dim).to(device)  # Шум для генератора\n        fake_data = generator(z)  # Генерация данных\n\n        # Фильтрация реальных данных \n        real_data_filtered = real_data[real_labels.squeeze() == 1]  # Только данные с меткой `1`\n        if real_data_filtered.size(0) == 0:  # Если нет пропускаем итерацию\n            continue\n\n        # Лейблы для дискриминатора\n        real_labels_filtered = torch.ones(real_data_filtered.size(0), 1).to(device)  \n        fake_labels = torch.zeros(batch_size, 1).to(device)  \n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data_filtered)  # Прогноз для реальных данных с меткой `1`\n        fake_output = discriminator(fake_data.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels_filtered)  # Используем только реальные метки `1`\n        d_loss_fake = criterion(fake_output, fake_labels)  # Метки для фейковых данных - всегда `0`\n        d_loss = d_loss_real + d_loss_fake\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # === Обучение генератора ===\n        fake_output = discriminator(fake_data)  # Генератор пытается обмануть дискриминатор\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Генератор хочет, чтобы дискриминатор принял фейковые данные за реальные\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T10:16:38.216771Z","iopub.execute_input":"2024-11-25T10:16:38.217422Z","iopub.status.idle":"2024-11-25T10:16:38.430064Z","shell.execute_reply.started":"2024-11-25T10:16:38.217387Z","shell.execute_reply":"2024-11-25T10:16:38.428748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ТУТ со штрафом","metadata":{}},{"cell_type":"code","source":"from torch.autograd import grad\n\n# Функция для расчета градиентного штрафа\ndef gradient_penalty(discriminator, real_data, fake_data, device):\n    batch_size = real_data.size(0)\n    epsilon = torch.rand(batch_size, 1, 1, device=device)\n    epsilon = epsilon.expand_as(real_data)\n    interpolates = epsilon * real_data + (1 - epsilon) * fake_data\n    interpolates.requires_grad_(True)\n\n    d_interpolates = discriminator(interpolates)\n    gradients = grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=torch.ones_like(d_interpolates),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    gradients = gradients.view(batch_size, -1)\n    gradient_norm = gradients.norm(2, dim=1)\n    gp = ((gradient_norm - 1) ** 2).mean()\n    return gp\n\n\nepochs = 6\nlatent_dim = 100\nlambda_gp = 10\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n\n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n\n    for real_data, real_labels in tqdm(train_loader):\n        # Перевод данных на устройство\n        real_data = real_data.to(device)\n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n\n        # Фильтрация реальных данных с меткой 1\n        mask = real_labels.squeeze() == 1\n        real_data_filtered = real_data[mask]\n        real_batch_size = real_data_filtered.size(0)\n\n        if real_batch_size == 0:  # Если нет данных с меткой 1, пропускаем итерацию\n            continue\n\n        # === Обучение дискриминатора ===\n        # Генерация фейковых данных для дискриминатора\n        z = torch.randn(real_batch_size, latent_dim).to(device)\n        fake_data = generator(z).detach()  # Отсоединяем граф для обучения дискриминатора\n\n        # Лейблы для дискриминатора\n        real_labels_filtered = torch.ones(real_batch_size, 1).to(device)  # Метки для реальных данных\n        fake_labels = torch.zeros(real_batch_size, 1).to(device)  # Метки для фейковых данных\n\n        # Градиентный штраф\n        gp = gradient_penalty(discriminator, real_data_filtered, fake_data, device)\n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data_filtered)\n        fake_output = discriminator(fake_data)\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels_filtered)\n        d_loss_fake = criterion(fake_output, fake_labels)\n        d_loss = d_loss_real + d_loss_fake + lambda_gp * gp\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # === Обучение генератора ===\n        # Генерация фейковых данных для генератора\n        z = torch.randn(real_batch_size, latent_dim).to(device)\n        fake_data = generator(z)  # Не используем .detach(), чтобы сохранить граф\n\n        # Прогноз дискриминатора для обновленных фейковых данных\n        fake_output = discriminator(fake_data)\n\n        # Лосс для генератора\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Генератор хочет, чтобы фейковые данные приняли за реальные\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T10:26:51.552802Z","iopub.execute_input":"2024-11-25T10:26:51.553103Z","iopub.status.idle":"2024-11-25T10:46:13.377169Z","shell.execute_reply.started":"2024-11-25T10:26:51.553077Z","shell.execute_reply":"2024-11-25T10:46:13.376154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\n\n\ndiscriminator.eval()\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n# Проверка на валидации\nall_labels = []\nall_scores = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n        real_data = real_data.to(device)  \n        labels = labels.to(device)\n\n        # Предсказания дискриминатора для реальных данных\n        real_output = discriminator(real_data)\n        all_scores.extend(real_output.cpu().numpy().flatten())  # Сохраняем вероятности\n        all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n\n# Расчёт EER\neer, threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\nprint(f\"Validation EER: {eer:.4f}, Threshold: {threshold:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T10:46:18.499723Z","iopub.execute_input":"2024-11-25T10:46:18.500071Z","iopub.status.idle":"2024-11-25T10:46:27.457701Z","shell.execute_reply.started":"2024-11-25T10:46:18.500042Z","shell.execute_reply":"2024-11-25T10:46:27.456861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ПРОБУЕМ С ШУМОМ В РЕАЛЬНЫХ ДАННЫХ","metadata":{}},{"cell_type":"code","source":"save_path = \"/kaggle/input/gan-v1/pytorch/default/4\"  \n\n# Загрузка весов\ngenerator = WaveGANGenerator(latent_dim=100).to(device)\ndiscriminator = WaveGANDiscriminator().to(device)\n\ngenerator.load_state_dict(torch.load(os.path.join(save_path, \"generator_checkpoint2.pth\")))\ndiscriminator.load_state_dict(torch.load(os.path.join(save_path, \"discriminator_checkpoint2.pth\")))\n\n# generator.eval()\n# discriminator.eval()\n\nprint(\"Model weights loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T10:09:53.727720Z","iopub.execute_input":"2024-12-02T10:09:53.728697Z","iopub.status.idle":"2024-12-02T10:09:54.410457Z","shell.execute_reply.started":"2024-12-02T10:09:53.728647Z","shell.execute_reply":"2024-12-02T10:09:54.409568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\nepochs = 5\nlatent_dim = 100\nnoise_std = 0.05  # Стандартное отклонение шума\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n\n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n    \n    for real_data, real_labels in tqdm(train_loader):\n        # Перевод данных на устройство\n        real_data = real_data.to(device)\n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Генерация фейковых данных на основе шума\n        z = torch.randn(batch_size, latent_dim).to(device)  # Шум для генератора\n        fake_data = generator(z)  # Генерация данных\n\n        # Фильтрация реальных данных \n        real_data_filtered = real_data[real_labels.squeeze() == 1]  # Только данные с меткой `1`\n        if real_data_filtered.size(0) == 0:  # Если нет, пропускаем итерацию\n            continue\n\n        # Добавление шума в реальные данные\n        noise = torch.randn_like(real_data_filtered) * noise_std\n        real_data_noisy = real_data_filtered + noise  # Добавляем шум к реальным данным\n        real_data_noisy = torch.clamp(real_data_noisy, min=0.0, max=1.0)  # Ограничиваем значения\n\n        # Лейблы для дискриминатора\n        real_labels_filtered = torch.ones(real_data_filtered.size(0), 1).to(device)  \n        fake_labels = torch.zeros(batch_size, 1).to(device)  \n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data_noisy)  # Прогноз для реальных данных с шумом\n        fake_output = discriminator(fake_data.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels_filtered)  # Используем только реальные метки `1`\n        d_loss_fake = criterion(fake_output, fake_labels)  # Метки для фейковых данных - всегда `0`\n        d_loss = d_loss_real + d_loss_fake\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Обучение генератора\n        fake_output = discriminator(fake_data)  # Генератор пытается обмануть дискриминатор\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Генератор хочет, чтобы дискриминатор принял фейковые данные за реальные\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:26:54.495962Z","iopub.execute_input":"2024-12-02T17:26:54.496693Z","iopub.status.idle":"2024-12-02T17:32:59.898746Z","shell.execute_reply.started":"2024-12-02T17:26:54.496657Z","shell.execute_reply":"2024-12-02T17:32:59.897853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\nepochs = 2\nlatent_dim = 100\nnoise_std = 0.05  # Стандартное отклонение шума\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n\n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n    \n    for real_data, real_labels in tqdm(train_loader):\n        # Перевод данных на устройство\n        real_data = real_data.to(device)\n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Генерация фейковых данных на основе шума\n        z = torch.randn(batch_size, latent_dim).to(device)  # Шум для генератора\n        fake_data = generator(z)  # Генерация данных\n\n\n        # Добавление шума в реальные данные\n        noise = torch.randn_like(real_data) * noise_std\n        real_data_noisy = real_data + noise  # Добавляем шум к реальным данным\n        real_data_noisy = torch.clamp(real_data_noisy, min=0.0, max=1.0)  # Ограничиваем значения\n\n        # Лейблы для дискриминатора\n        real_labels_filtered = torch.ones(real_data_filtered.size(0), 1).to(device)  \n        fake_labels = torch.zeros(batch_size, 1).to(device)  \n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data_noisy)  # Прогноз для реальных данных с шумом\n        fake_output = discriminator(fake_data.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels)  # Используем только реальные метки `1`\n        d_loss_fake = criterion(fake_output, fake_labels)  # Метки для фейковых данных - всегда `0`\n        d_loss = (d_loss_real + d_loss_fake)/2\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Обучение генератора\n        fake_output = discriminator(fake_data)  # Генератор пытается обмануть дискриминатор\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Генератор хочет, чтобы дискриминатор принял фейковые данные за реальные\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T21:37:16.732049Z","iopub.execute_input":"2024-12-02T21:37:16.733096Z","iopub.status.idle":"2024-12-02T21:57:20.015507Z","shell.execute_reply.started":"2024-12-02T21:37:16.733059Z","shell.execute_reply":"2024-12-02T21:57:20.014445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nepochs = 1\nlatent_dim = 100\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n\n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n    \n    for real_data, real_labels in tqdm(train_loader):\n        # Перевод данных на устройство\n        real_data = real_data.to(device)\n       \n        \n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Генерация фейковых данных на основе шума\n        z = torch.randn(batch_size, latent_dim).to(device)  # Шум для генератора\n        fake_data = generator(z)  # Генерация данных\n\n\n\n        # Лейблы для дискриминатора \n        fake_labels = torch.zeros(batch_size, 1).to(device)  \n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data)  # Прогноз для реальных данных с шумом\n        fake_output = discriminator(fake_data.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels)  # Используем только реальные метки `1`\n        d_loss_fake = criterion(fake_output, fake_labels)  # Метки для фейковых данных - всегда `0`\n        d_loss = (d_loss_real + d_loss_fake)/2\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Обучение генератора\n        fake_output = discriminator(fake_data)  # Генератор пытается обмануть дискриминатор\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Генератор хочет, чтобы дискриминатор принял фейковые данные за реальные\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T14:32:43.150486Z","iopub.execute_input":"2024-12-05T14:32:43.151096Z","iopub.status.idle":"2024-12-05T14:55:32.497198Z","shell.execute_reply.started":"2024-12-05T14:32:43.151060Z","shell.execute_reply":"2024-12-05T14:55:32.496270Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"С ШУМОМ","metadata":{}},{"cell_type":"code","source":"epochs = 5\nlatent_dim = 100\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n    \n    for real_data, real_labels in tqdm(train_loader):\n        # Перевод данных на устройство\n        real_data = real_data.to(device)\n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Генерация фейковых данных на основе шума\n        z = torch.randn(batch_size, latent_dim).to(device)  # Шум для генератора\n        fake_data = generator(z)  # Генерация данных\n\n        # Добавляем шум в данные (без in-place модификации)\n        real_data_noisy = real_data + 0.9 * torch.randn_like(real_data)  # Шум для реальных данных\n        fake_data_noisy = fake_data + 0.00005 * torch.randn_like(fake_data)  # Шум для фейковых данных\n\n        # Лейблы для дискриминатора \n        fake_labels = torch.zeros(batch_size, 1).to(device)  \n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data_noisy)  # Прогноз для реальных данных с шумом\n        fake_output = discriminator(fake_data_noisy.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels)  # Используем только реальные метки `1`\n        d_loss_fake = criterion(fake_output, fake_labels)  # Метки для фейковых данных - всегда `0`\n        d_loss = (d_loss_real + d_loss_fake) / 2\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Обучение генератора\n        fake_output = discriminator(fake_data_noisy)  # Генератор пытается обмануть дискриминатор\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Генератор хочет, чтобы дискриминатор принял фейковые данные за реальные\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:32:58.382817Z","iopub.execute_input":"2024-12-04T19:32:58.383168Z","iopub.status.idle":"2024-12-04T20:14:46.960264Z","shell.execute_reply.started":"2024-12-04T19:32:58.383136Z","shell.execute_reply":"2024-12-04T20:14:46.959174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T17:42:45.451193Z","iopub.execute_input":"2024-12-02T17:42:45.452069Z","iopub.status.idle":"2024-12-02T17:42:45.474184Z","shell.execute_reply.started":"2024-12-02T17:42:45.452026Z","shell.execute_reply":"2024-12-02T17:42:45.473066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\n\n\ndiscriminator.eval()\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n# Проверка на валидации\nall_labels = []\nall_scores = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n        real_data = real_data.to(device)  \n        labels = labels.to(device)\n\n        # Предсказания дискриминатора для реальных данных\n        real_output = discriminator(real_data)\n        all_scores.extend(real_output.cpu().numpy().flatten())  # Сохраняем вероятности\n        all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n\n# Расчёт EER\neer, threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\nprint(f\"Validation EER: {eer:.4f}, Threshold: {threshold:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:01:25.600738Z","iopub.execute_input":"2024-11-27T18:01:25.601445Z","iopub.status.idle":"2024-11-27T18:01:33.143536Z","shell.execute_reply.started":"2024-11-27T18:01:25.601415Z","shell.execute_reply":"2024-11-27T18:01:33.142617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, f1_score, accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\n\ndiscriminator.eval()\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n# Проверка на валидации\nall_labels = []\nall_scores = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n        real_data = real_data.to(device)  \n        labels = labels.to(device)\n\n        # Предсказания дискриминатора для реальных данных\n        real_output = discriminator(real_data)\n        all_scores.extend(real_output.cpu().numpy().flatten())  # Сохраняем вероятности\n        all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n\n# Преобразуем вероятности в предсказанные классы\nthreshold = 0.5  # Порог по умолчанию\npredicted_classes = np.array(all_scores) >= threshold\n\n# Расчёт метрик\neer, eer_threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\nf1 = f1_score(all_labels, predicted_classes)\naccuracy = accuracy_score(all_labels, predicted_classes)\n\n# Вывод результатов\nprint(f\"Validation Metrics:\")\nprint(f\"  EER: {eer:.4f}, Threshold: {eer_threshold:.4f}\")\nprint(f\"  F1 Score: {f1:.4f}\")\nprint(f\"  Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:36:07.480370Z","iopub.execute_input":"2024-12-12T14:36:07.481191Z","iopub.status.idle":"2024-12-12T14:39:30.952990Z","shell.execute_reply.started":"2024-12-12T14:36:07.481158Z","shell.execute_reply":"2024-12-12T14:39:30.952187Z"}},"outputs":[{"name":"stderr","text":"Validation: 100%|██████████| 1553/1553 [03:23<00:00,  7.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Metrics:\n  EER: 0.0506, Threshold: 0.9180\n  F1 Score: 0.6417\n  Accuracy: 0.8874\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"sum(all_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:04:02.723368Z","iopub.execute_input":"2024-12-05T15:04:02.723655Z","iopub.status.idle":"2024-12-05T15:04:02.730334Z","shell.execute_reply.started":"2024-12-05T15:04:02.723628Z","shell.execute_reply":"2024-12-05T15:04:02.729392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**exp1**\nsum(all_scores) 1.682265730237655e-36\nValidation Metrics:\r\n  EER: 0.0237, Threshold: 0.0000\r\n  F1 Score: 0.0000\r\n  Accuracy: 0.9\n\n**exp2**\n8.695440772771884e-37\nValidation Metrics:\r\n  EER: 0.0185, Threshold: 0.0000\r\n  F1 Score: 0.0000\r\n  Accuracy: 0.9\n\n**exp3**\n\n\n\n**НОВАЯ АРХИТЕКТУРА**\n\nБЕЗ ШУМА ЭКСПЕРИМЕНТЫ\n\n**exp1**\n\n2.0535277089509515e-19\n\nValidation Metrics:\r\n  EER: 0.3837, Threshold: 0.0000\r\n  F1 Score: 0.0000\r\n  Accuracy: 0.9\n\n\n**exp2**\n\n3.747425772844711\n\nValidation Metrics:\r\n  EER: 0.0489, Threshold: 0.0000\r\n  F1 Score: 0.0392\r\n  Accuracy: 0.9\n\n\n**ПОСЛЕ БАЛАНСА**\nValidation Metrics:\r\n  EER: 0.0067, Threshold: 0.0000\r\n  F1 Score: 0.2655\r\n  Accuracy: 0.5\n\n\n\n**ВЕРСИЯ 3**\n\nValidation Metrics:\r\n  EER: 0.1056, Threshold: 0.0000\r\n  F1 Score: 0.5110\r\n  Accuracy: 0.9\n\nС БАЛАНСОМ\nValidation Metrics:\r\n  EER: 0.0742, Threshold: 0.0000\r\n  F1 Score: 0.5354\r\n  Accuracy: 0.6\n\n**ЭКСПЕРИМЕНТ 2** v3_check2\n\n\nValidation Metrics:\r\n  EER: 0.0467, Threshold: 0.0000\r\n  F1 Score: 0.5582\r\n  Accuracy: 0.6\n\n\n**ЭКСПЕРИМЕНТ 3** v3_check3\n\n\nValidation Metrics:\r\n  EER: 0.0589, Threshold: 0.0000\r\n  F1 Score: 0.2968\r\n  Accuracy: 0.5\n\n\n\nValidation Metrics:\r\n  EER: 0.0573, Threshold: 0.0000\r\n  F1 Score: 0.2526\r\n  Accuracy: 0.5\n\n\n**ЭКСПЕРИМЕНТ 4** v3_check5\n\nValidation Metrics:\r\n  EER: 0.0569, Threshold: 0.0000\r\n  F1 Score: 0.3333\r\n  Accuracy: 0.5\n\n\n\n**ПОСЛЕ ПРУНИНГА** ВЕСА БЫЛИ v3_check2\n\nEER: 0.0860, Threshold: 0.0006\r\n  F1 Score: 0.5507\r\n  Accuracy: 0.93\n\n\n**ПОСЛЕ ПРУНИНГА2** ВЕСА БЫЛИ v3_check2\n\nEER: 0.0823, Threshold: 0.0003\r\n  F1 Score: 0.5038\r\n  Accuracy: 0.931\n\n\n**ПОСЛЕ ПУРИНГА И ТЮНИНГА ВЕСА plus 2**\n\nValidation Metrics:\n\r\n  EER: 0.0399, Threshold: 0.841\n  4\r\n\n**ПОСЛЕ ПУРИНГА И ТЮНИНГА ВЕСА plus 3**\n\n\n\n**ПОСЛЕ ПУРИНГА И ТЮНИНГА ВЕСА plus 4**\nValidation Metrics:\r\n  EER: 0.0434, Threshold: 0.7412\r\n  F1 Score: 0.7789\r\n  Accuracy: 0.9\n\n\n**ПОСЛЕ ПУРИНГА 3**\nValidation Metrics:\r\n  EER: 0.0506, Threshold: 0.9180\r\n  F1 Score: 0.6417\r\n  Accuracy: 0.88744354000\n80989714861925792288765020000000000","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сохраняем веса после обучения\ntorch.save(generator.state_dict(), \"generator_prune_plus4.pth\")\ntorch.save(discriminator.state_dict(), \"discriminator_prune_plus4.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:49:54.878745Z","iopub.execute_input":"2024-12-12T12:49:54.879655Z","iopub.status.idle":"2024-12-12T12:49:54.953668Z","shell.execute_reply.started":"2024-12-12T12:49:54.879619Z","shell.execute_reply":"2024-12-12T12:49:54.952752Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/generator_prune_plus4.pth\"\n\n\ndisplay(FileLink(\"generator_prune_plus4.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:50:02.693423Z","iopub.execute_input":"2024-12-12T12:50:02.693802Z","iopub.status.idle":"2024-12-12T12:50:02.700348Z","shell.execute_reply.started":"2024-12-12T12:50:02.693774Z","shell.execute_reply":"2024-12-12T12:50:02.699479Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/generator_prune_plus4.pth","text/html":"<a href='generator_prune_plus4.pth' target='_blank'>generator_prune_plus4.pth</a><br>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/discriminator_prune_plus4.pth\"\n\n\ndisplay(FileLink(\"discriminator_prune_plus4.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:50:03.390052Z","iopub.execute_input":"2024-12-12T12:50:03.390684Z","iopub.status.idle":"2024-12-12T12:50:03.395865Z","shell.execute_reply.started":"2024-12-12T12:50:03.390656Z","shell.execute_reply":"2024-12-12T12:50:03.395060Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/discriminator_prune_plus4.pth","text/html":"<a href='discriminator_prune_plus4.pth' target='_blank'>discriminator_prune_plus4.pth</a><br>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import csv \nfrom tqdm import tqdm\nimport os\nimport torch\nimport librosa\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:34:16.784372Z","iopub.execute_input":"2024-11-27T17:34:16.785137Z","iopub.status.idle":"2024-11-27T17:34:16.789050Z","shell.execute_reply.started":"2024-11-27T17:34:16.785103Z","shell.execute_reply":"2024-11-27T17:34:16.788323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"/kaggle/input/gan_v3/pytorch/default/6\"  # Путь для сохранения весов\n\n# Загрузка весов\ndiscriminator = WaveGANDiscriminator().to(device)\n\ndiscriminator.load_state_dict(torch.load(os.path.join(save_path, \"pruned_discriminator2.pth\")))\n\ndiscriminator.eval()\n\nprint(\"Model weights loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T19:30:16.171120Z","iopub.execute_input":"2024-12-09T19:30:16.171519Z","iopub.status.idle":"2024-12-09T19:30:16.280336Z","shell.execute_reply.started":"2024-12-09T19:30:16.171476Z","shell.execute_reply":"2024-12-09T19:30:16.279262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"/kaggle/input/gan_v3/pytorch/default/9\"  # Путь для сохранения весов\n\n# Загрузка весов\ngenerator = WaveGANGenerator(latent_dim=100).to(device)\ndiscriminator = WaveGANDiscriminator().to(device)\n\ngenerator.load_state_dict(torch.load(os.path.join(save_path, \"generator_prune_plus3.pth\")))\ndiscriminator.load_state_dict(torch.load(os.path.join(save_path, \"discriminator_prune_plus3.pth\")))\n\ngenerator.eval()\ndiscriminator.eval()\n\nprint(\"Model weights loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:36:54.187295Z","iopub.execute_input":"2024-12-12T11:36:54.188185Z","iopub.status.idle":"2024-12-12T11:36:54.805503Z","shell.execute_reply.started":"2024-12-12T11:36:54.188147Z","shell.execute_reply":"2024-12-12T11:36:54.804330Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1360113039.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  generator.load_state_dict(torch.load(os.path.join(save_path, \"generator_prune_plus3.pth\")))\n/tmp/ipykernel_30/1360113039.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  discriminator.load_state_dict(torch.load(os.path.join(save_path, \"discriminator_prune_plus3.pth\")))\n","output_type":"stream"},{"name":"stdout","text":"Model weights loaded successfully.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"save_path = \"/kaggle/working\"  # Путь для сохранения весов\n\n# Загрузка весов\ngenerator = WaveGANGenerator(latent_dim=100).to(device)\ndiscriminator = WaveGANDiscriminator().to(device)\n\ngenerator.load_state_dict(torch.load(os.path.join(save_path, \"generator_v3_check2.pth\")))\ndiscriminator.load_state_dict(torch.load(os.path.join(save_path, \"discriminator_v3_check2.pth\")))\n\ngenerator.eval()\ndiscriminator.eval()\n\nprint(\"Model weights loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:15:46.677128Z","iopub.execute_input":"2024-12-04T20:15:46.677490Z","iopub.status.idle":"2024-12-04T20:15:46.830754Z","shell.execute_reply.started":"2024-12-04T20:15:46.677458Z","shell.execute_reply":"2024-12-04T20:15:46.829901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv \nfrom tqdm import tqdm\nimport os\nimport torch\nimport librosa\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nimport numpy as np\n\ndef preprocess_audio(file_path, target_length=64600):\n    \"\"\"\n    Загружает аудио, приводит его к фиксированной длине и возвращает тензор [1, T].\n    \"\"\"\n    # Загрузка аудио\n    audio, sr = librosa.load(file_path, sr=None)\n\n    # Приведение к фиксированной длине\n    if len(audio) > target_length:\n        audio = audio[:target_length]\n    else:\n        padding = target_length - len(audio)\n        audio = np.pad(audio, (0, padding), mode=\"constant\")\n\n    # Преобразование в Tensor с добавлением измерения канала\n    audio = torch.tensor(audio, dtype=torch.float).unsqueeze(0)  # [1, T]\n    return audio\n\n\ndef predict_competition(discriminator, test_path, device, submission_path=\"/kaggle/working/submission_GAN.csv\"):\n\n    discriminator.eval()  # Переводим дискриминатор в режим оценки\n    submission = []\n\n    for file_name in tqdm(os.listdir(test_path), desc=\"Predicting competition data\"):\n        file_path = os.path.join(test_path, file_name)\n\n        # Предобработка аудио\n        audio_tensor = preprocess_audio(file_path).to(device)\n\n        with torch.no_grad():\n            # Прогон через дискриминатор\n            output = discriminator(audio_tensor.unsqueeze(1))  # Добавляем размер канала [1, T] -> [1, 1, T]\n            score = output.item()  # Получаем предсказанную вероятность\n\n        submission.append((file_name.split('.')[0], score))\n\n    # Сохраняем предсказания в файл\n    with open(submission_path, \"w\", newline=\"\") as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"ID\", \"score\"])\n        writer.writerows(submission)\n\n    print(f\"Submission file saved to {submission_path}\")\n    return submission_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:04:19.186488Z","iopub.execute_input":"2024-12-12T13:04:19.187107Z","iopub.status.idle":"2024-12-12T13:04:19.196036Z","shell.execute_reply.started":"2024-12-12T13:04:19.187071Z","shell.execute_reply":"2024-12-12T13:04:19.195118Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"test_path = \"/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs\"\n\n# Выполняем предсказания\nsubmission_file = predict_competition(discriminator, test_path, device)\nprint(f\"Submission file created at: {submission_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:04:19.635623Z","iopub.execute_input":"2024-12-12T13:04:19.635910Z","iopub.status.idle":"2024-12-12T13:53:02.517304Z","shell.execute_reply.started":"2024-12-12T13:04:19.635881Z","shell.execute_reply":"2024-12-12T13:53:02.516349Z"}},"outputs":[{"name":"stderr","text":"Predicting competition data: 100%|██████████| 144693/144693 [48:37<00:00, 49.60it/s] \n","output_type":"stream"},{"name":"stdout","text":"Submission file saved to /kaggle/working/submission_GAN.csv\nSubmission file created at: /kaggle/working/submission_GAN.csv\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### Дообучение после прунинга","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, f1_score, accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\n\ndiscriminator.eval()\n\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n    if len(np.unique(y_true)) < 2:\n        raise ValueError(\"y_true must contain both classes (0 and 1).\")\n    \n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    \n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer, eer_threshold\n\n\ndef eval_2():\n    # Проверка на валидации\n    all_labels = []\n    all_scores = []\n    \n    with torch.no_grad():\n        for real_data, labels in tqdm(val_loader, desc=\"Validation\"):\n            real_data = real_data.to(device)  \n            labels = labels.to(device)\n    \n            # Предсказания дискриминатора для реальных данных\n            real_output = discriminator(real_data)\n            all_scores.extend(real_output.cpu().numpy().flatten())  # Сохраняем вероятности\n            all_labels.extend(labels.cpu().numpy())  # Сохраняем истинные метки\n    \n    # Преобразуем вероятности в предсказанные классы\n    threshold = 0.5  # Порог по умолчанию\n    predicted_classes = np.array(all_scores) >= threshold\n    \n    # Расчёт метрик\n    eer, eer_threshold = calculate_eer(np.array(all_labels), np.array(all_scores))\n    f1 = f1_score(all_labels, predicted_classes)\n    accuracy = accuracy_score(all_labels, predicted_classes)\n    \n    # Вывод результатов\n    print(f\"Validation Metrics:\")\n    print(f\"  EER: {eer:.4f}, Threshold: {eer_threshold:.4f}\")\n    print(f\"  F1 Score: {f1:.4f}\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:37:03.470212Z","iopub.execute_input":"2024-12-12T11:37:03.470664Z","iopub.status.idle":"2024-12-12T11:37:03.479378Z","shell.execute_reply.started":"2024-12-12T11:37:03.470629Z","shell.execute_reply":"2024-12-12T11:37:03.478591Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nepochs = 3\nlatent_dim = 100\n\ngenerator.train()\ndiscriminator.train()\n\nfor epoch in range(epochs):\n\n    d_loss_real_total = 0\n    d_loss_fake_total = 0\n    num_batches = 0\n    \n    for real_data, real_labels in tqdm(train_loader):\n        # Перевод данных на устройство\n        real_data = real_data.to(device)\n       \n        \n        real_labels = real_labels.to(device).float().unsqueeze(1)  # Лейблы (0 или 1)\n        batch_size = real_data.size(0)\n\n        # Генерация фейковых данных на основе шума\n        z = torch.randn(batch_size, latent_dim).to(device)  # Шум для генератора\n        fake_data = generator(z)  # Генерация данных\n\n\n\n        # Лейблы для дискриминатора \n        fake_labels = torch.zeros(batch_size, 1).to(device)  \n\n        # Прогноз дискриминатора\n        real_output = discriminator(real_data)  # Прогноз для реальных данных с шумом\n        fake_output = discriminator(fake_data.detach())  # Прогноз для сгенерированных данных\n\n        # Лосс для дискриминатора\n        d_loss_real = criterion(real_output, real_labels)  # Используем только реальные метки `1`\n        d_loss_fake = criterion(fake_output, fake_labels)  # Метки для фейковых данных - всегда `0`\n        d_loss = (d_loss_real + d_loss_fake)/2\n\n        # Шаг оптимизации дискриминатора\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Обучение генератора\n        fake_output = discriminator(fake_data)  # Генератор пытается обмануть дискриминатор\n        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # Генератор хочет, чтобы дискриминатор принял фейковые данные за реальные\n\n        # Шаг оптимизации генератора\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Суммируем лоссы для подсчета среднего\n        d_loss_real_total += d_loss_real.item()\n        d_loss_fake_total += d_loss_fake.item()\n        num_batches += 1\n\n    # Вычисление средних значений\n    d_loss_real_avg = d_loss_real_total / num_batches\n    d_loss_fake_avg = d_loss_fake_total / num_batches\n\n    print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n          f\"D Loss Real Avg: {d_loss_real_avg:.4f}, D Loss Fake Avg: {d_loss_fake_avg:.4f}\")\n    \n    eval_2()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:37:09.584847Z","iopub.execute_input":"2024-12-12T11:37:09.585179Z","iopub.status.idle":"2024-12-12T12:48:33.359978Z","shell.execute_reply.started":"2024-12-12T11:37:09.585148Z","shell.execute_reply":"2024-12-12T12:48:33.358883Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 3139/3139 [28:25<00:00,  1.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], D Loss: 0.0005, G Loss: 12.8715, D Loss Real Avg: 0.1256, D Loss Fake Avg: 0.0015\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 125/125 [00:10<00:00, 11.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Metrics:\n  EER: 0.0643, Threshold: 0.0152\n  F1 Score: 0.8146\n  Accuracy: 0.9620\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3139/3139 [20:57<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3], D Loss: 0.1506, G Loss: 13.1910, D Loss Real Avg: 0.1257, D Loss Fake Avg: 0.0016\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 125/125 [00:10<00:00, 12.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Metrics:\n  EER: 0.0621, Threshold: 0.0206\n  F1 Score: 0.8180\n  Accuracy: 0.9635\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3139/3139 [21:29<00:00,  2.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3], D Loss: 0.0994, G Loss: 12.3311, D Loss Real Avg: 0.1275, D Loss Fake Avg: 0.0016\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 125/125 [00:10<00:00, 12.21it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Metrics:\n  EER: 0.0643, Threshold: 0.0160\n  F1 Score: 0.8077\n  Accuracy: 0.9600\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Повышение производительности модели","metadata":{}},{"cell_type":"markdown","source":"## Прунинг","metadata":{}},{"cell_type":"markdown","source":"Для облегчения модели реализуем прунинг.","metadata":{}},{"cell_type":"markdown","source":"Визуализация распределения весов модели.\n\nФункция ниже строит гистограммы распределения весов обучаемых параметров модели,\nпозволяя понять, как распределены веса в разных слоях нейронной сети. ","metadata":{}},{"cell_type":"code","source":"for name, module in discriminator.named_modules():\n    print(name, module)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:33:06.288318Z","iopub.execute_input":"2024-12-12T14:33:06.288694Z","iopub.status.idle":"2024-12-12T14:33:06.294583Z","shell.execute_reply.started":"2024-12-12T14:33:06.288661Z","shell.execute_reply":"2024-12-12T14:33:06.293563Z"}},"outputs":[{"name":"stdout","text":" WaveGANDiscriminator(\n  (model): Sequential(\n    (0): Conv1d(1, 64, kernel_size=(25,), stride=(4,), padding=(11,))\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Dropout(p=0.3, inplace=False)\n    (3): Conv1d(64, 128, kernel_size=(25,), stride=(4,), padding=(11,))\n    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.2)\n    (6): Dropout(p=0.3, inplace=False)\n    (7): Conv1d(128, 256, kernel_size=(25,), stride=(4,), padding=(11,))\n    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): LeakyReLU(negative_slope=0.2)\n    (10): Dropout(p=0.3, inplace=False)\n    (11): Conv1d(256, 512, kernel_size=(25,), stride=(4,), padding=(11,))\n    (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (13): LeakyReLU(negative_slope=0.2)\n    (14): Dropout(p=0.3, inplace=False)\n    (15): Flatten(start_dim=1, end_dim=-1)\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=129024, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n)\nmodel Sequential(\n  (0): Conv1d(1, 64, kernel_size=(25,), stride=(4,), padding=(11,))\n  (1): LeakyReLU(negative_slope=0.2)\n  (2): Dropout(p=0.3, inplace=False)\n  (3): Conv1d(64, 128, kernel_size=(25,), stride=(4,), padding=(11,))\n  (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): LeakyReLU(negative_slope=0.2)\n  (6): Dropout(p=0.3, inplace=False)\n  (7): Conv1d(128, 256, kernel_size=(25,), stride=(4,), padding=(11,))\n  (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (9): LeakyReLU(negative_slope=0.2)\n  (10): Dropout(p=0.3, inplace=False)\n  (11): Conv1d(256, 512, kernel_size=(25,), stride=(4,), padding=(11,))\n  (12): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (13): LeakyReLU(negative_slope=0.2)\n  (14): Dropout(p=0.3, inplace=False)\n  (15): Flatten(start_dim=1, end_dim=-1)\n)\nmodel.0 Conv1d(1, 64, kernel_size=(25,), stride=(4,), padding=(11,))\nmodel.1 LeakyReLU(negative_slope=0.2)\nmodel.2 Dropout(p=0.3, inplace=False)\nmodel.3 Conv1d(64, 128, kernel_size=(25,), stride=(4,), padding=(11,))\nmodel.4 BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nmodel.5 LeakyReLU(negative_slope=0.2)\nmodel.6 Dropout(p=0.3, inplace=False)\nmodel.7 Conv1d(128, 256, kernel_size=(25,), stride=(4,), padding=(11,))\nmodel.8 BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nmodel.9 LeakyReLU(negative_slope=0.2)\nmodel.10 Dropout(p=0.3, inplace=False)\nmodel.11 Conv1d(256, 512, kernel_size=(25,), stride=(4,), padding=(11,))\nmodel.12 BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nmodel.13 LeakyReLU(negative_slope=0.2)\nmodel.14 Dropout(p=0.3, inplace=False)\nmodel.15 Flatten(start_dim=1, end_dim=-1)\nfc Sequential(\n  (0): Linear(in_features=129024, out_features=1, bias=True)\n  (1): Sigmoid()\n)\nfc.0 Linear(in_features=129024, out_features=1, bias=True)\nfc.1 Sigmoid()\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def plot_num_parameters_distribution(model):\n    num_parameters = dict()\n    for name, param in model.named_parameters():\n        if param.dim() > 1:\n            num_parameters[name] = param.numel()\n    fig = plt.figure(figsize=(8, 6))\n    plt.grid(axis='y')\n    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n    plt.title('#Parameter Distribution')\n    plt.ylabel('Number of Parameters')\n    plt.xticks(rotation=60)\n    plt.tight_layout()\n    plt.show()\n\nplot_num_parameters_distribution(discriminator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:33:06.758350Z","iopub.execute_input":"2024-12-12T14:33:06.759054Z","iopub.status.idle":"2024-12-12T14:33:07.080300Z","shell.execute_reply.started":"2024-12-12T14:33:06.759024Z","shell.execute_reply":"2024-12-12T14:33:07.079606Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_weight_distribution(model, bins=256, count_nonzero_only=False):\n    fig, axes = plt.subplots(3, 3, figsize=(12, 8))  \n    axes = axes.ravel()\n    plot_index = 0\n\n    for name, param in model.named_parameters():\n        if param.dim() > 1:  # Анализируем только многомерные параметры (веса слоев)\n            ax = axes[plot_index]\n            if count_nonzero_only:\n                param_cpu = param.detach().view(-1).cpu()\n                param_cpu = param_cpu[param_cpu != 0].view(-1)\n                ax.hist(param_cpu, bins=bins, density=True, color='blue', alpha=0.5)\n            else:\n                ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True, color='blue', alpha=0.5)\n            ax.set_xlabel(name)\n            ax.set_ylabel('Density')\n            plot_index += 1\n\n            if plot_index >= len(axes):  # Прекращаем, если слоев больше, чем ячеек\n                break\n\n    fig.suptitle('Histogram of Weights', fontsize=16)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    plt.show()\n\n\nplot_weight_distribution(discriminator)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:33:17.446654Z","iopub.execute_input":"2024-12-12T14:33:17.447347Z","iopub.status.idle":"2024-12-12T14:33:56.641315Z","shell.execute_reply.started":"2024-12-12T14:33:17.447314Z","shell.execute_reply":"2024-12-12T14:33:56.639914Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     fig\u001b[38;5;241m.\u001b[39msubplots_adjust(top\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     27\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mplot_weight_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mplot_weight_distribution\u001b[0;34m(model, bins, count_nonzero_only)\u001b[0m\n\u001b[1;32m     14\u001b[0m     ax\u001b[38;5;241m.\u001b[39mhist(param_cpu, bins\u001b[38;5;241m=\u001b[39mbins, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(name)\n\u001b[1;32m     18\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDensity\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1450\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_axes.py:6704\u001b[0m, in \u001b[0;36mAxes.hist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6701\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   6703\u001b[0m \u001b[38;5;66;03m# Massage 'x' for processing.\u001b[39;00m\n\u001b[0;32m-> 6704\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reshape_2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6705\u001b[0m nx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)  \u001b[38;5;66;03m# number of datasets\u001b[39;00m\n\u001b[1;32m   6707\u001b[0m \u001b[38;5;66;03m# Process unit information.  _process_unit_info sets the unit and\u001b[39;00m\n\u001b[1;32m   6708\u001b[0m \u001b[38;5;66;03m# converts the first dataset; then we convert each following dataset\u001b[39;00m\n\u001b[1;32m   6709\u001b[0m \u001b[38;5;66;03m# one at a time.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1409\u001b[0m, in \u001b[0;36m_reshape_2D\u001b[0;34m(X, name)\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1408\u001b[0m         is_1d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1409\u001b[0m xi \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m nd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndim(xi)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nd \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1079\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m __array_priority__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# prefer Tensor ops over numpy ones\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1081\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stdout","text":"Error in callback <function flush_figures at 0x79aa1559b880> (for post_execute), with arguments args (),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib_inline/backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m InlineBackend\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mclose_figures:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         ip \u001b[38;5;241m=\u001b[39m get_ipython()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:457\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    459\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    460\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:400\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    398\u001b[0m      (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    399\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:1395\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;66;03m# Shift label away from axes to avoid overlapping ticklabels.\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_offset_text_position(tlb1, tlb2)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:2363\u001b[0m, in \u001b[0;36mXAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   2361\u001b[0m     \u001b[38;5;66;03m# use Axes if spine doesn't exist\u001b[39;00m\n\u001b[1;32m   2362\u001b[0m     spinebbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox\n\u001b[0;32m-> 2363\u001b[0m bbox \u001b[38;5;241m=\u001b[39m \u001b[43mmtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mspinebbox\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2364\u001b[0m bottom \u001b[38;5;241m=\u001b[39m bbox\u001b[38;5;241m.\u001b[39my0\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mset_position(\n\u001b[1;32m   2367\u001b[0m     (x, bottom \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabelpad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m72\u001b[39m)\n\u001b[1;32m   2368\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/transforms.py:654\u001b[0m, in \u001b[0;36mBboxBase.union\u001b[0;34m(bboxes)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bboxes):\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 654\u001b[0m x0 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxmin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m x1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax([bbox\u001b[38;5;241m.\u001b[39mxmax \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes])\n\u001b[1;32m    656\u001b[0m y0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin([bbox\u001b[38;5;241m.\u001b[39mymin \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2953\u001b[0m, in \u001b[0;36mmin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[1;32m   2837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2838\u001b[0m         where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2839\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2840\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2841\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2954\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":29},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"определения влияния разреженности слоев модели на точность","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\n\n# Функция для применения разреженности к отдельному слою\n@torch.no_grad()\ndef fine_grained_prune(param, sparsity):\n    \"\"\"\n    Применяет разреженность (прунинг) к параметру слоя.\n    \"\"\"\n    num_params = param.numel()\n    num_zero = int(num_params * sparsity)  # Количество обнуляемых весов\n\n    # Определяем пороговое значение для обнуления\n    threshold = torch.topk(param.abs().view(-1), num_zero, largest=False).values.max()\n\n    # Создаем маску и применяем разрежение\n    mask = (param.abs() > threshold).float()\n    param.mul_(mask)  # Применяем маску in-place\n    return mask\n\n# Функция оценки точности\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        for data, labels in tqdm(dataloader, desc=\"Evaluating Model\", leave=False):\n            data, labels = data.to(device), labels.to(device)\n            outputs = model(data)\n            preds = (outputs > 0.5).float().cpu().numpy()\n            all_preds.extend(preds.flatten())\n            all_labels.extend(labels.cpu().numpy())\n    return accuracy_score(all_labels, all_preds)\n\n# Анализ влияния разреженности на точность\n@torch.no_grad()\ndef sensitivity_analysis(model, dataloader, sparsity_values):\n    \"\"\"\n    Проводит анализ влияния разреженности слоев модели на точность.\n    \"\"\"\n    # Сохраняем исходное состояние параметров\n    original_state = {name: param.clone() for name, param in model.named_parameters() if param.dim() > 1}\n\n    sensitivity_results = {}\n    for name, param in tqdm(model.named_parameters(), desc=\"Analyzing Layers\"):\n        if param.dim() > 1:  # Только для весов с размерностью больше 1\n            accuracies = []\n            for sparsity in sparsity_values:\n                # Разрежаем слой\n                print(f\"Pruning {name} with sparsity {sparsity}...\")\n                fine_grained_prune(param, sparsity)\n                # Оцениваем точность\n                accuracy = evaluate_model(model, dataloader)\n                accuracies.append(accuracy)\n                print(f\"Accuracy after pruning {name} at sparsity {sparsity}: {accuracy:.4f}\")\n                # Восстанавливаем параметры слоя\n                param.data.copy_(original_state[name])\n\n            sensitivity_results[name] = accuracies\n            print(f\"Layer {name} analysis completed.\")\n    return sensitivity_results\n\n# Настройки анализа\nsparsity_values = [0.1, 0.3, 0.5, 0.7, 0.9]  # Уровни разреженности\n# Уменьшение объема данных для теста\nsmall_val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Запуск анализа\nsensitivity_results = sensitivity_analysis(discriminator, small_val_loader, sparsity_values)\n\n# Визуализация результатов\nimport matplotlib.pyplot as plt\n\nfor layer_name, accuracies in sensitivity_results.items():\n    plt.plot(sparsity_values, accuracies, label=layer_name)\n\nplt.xlabel(\"Sparsity\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Sensitivity Analysis: Layer Sparsity vs Accuracy\")\nplt.legend()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:16:48.460021Z","iopub.execute_input":"2024-12-09T09:16:48.460619Z","iopub.status.idle":"2024-12-09T09:21:35.450854Z","shell.execute_reply.started":"2024-12-09T09:16:48.460585Z","shell.execute_reply":"2024-12-09T09:21:35.450082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\n\n# Функция для применения разреженности к отдельному слою\n@torch.no_grad()\ndef fine_grained_prune(param, sparsity):\n    \"\"\"\n    Применяет разреженность (прунинг) к параметру слоя.\n    \"\"\"\n    num_params = param.numel()\n    num_zero = int(num_params * sparsity)  # Количество обнуляемых весов\n\n    # Определяем пороговое значение для обнуления\n    threshold = torch.topk(param.abs().view(-1), num_zero, largest=False).values.max()\n\n    # Создаем маску и применяем разрежение\n    mask = (param.abs() > threshold).float()\n    param.mul_(mask)  # Применяем маску in-place\n    return mask\n\n# Функция оценки F1-метрики\ndef evaluate_model_f1(model, dataloader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        for data, labels in tqdm(dataloader, desc=\"Evaluating Model\", leave=False):\n            data, labels = data.to(device), labels.to(device)\n            outputs = model(data)\n            preds = (outputs > 0.5).float().cpu().numpy()\n            all_preds.extend(preds.flatten())\n            all_labels.extend(labels.cpu().numpy())\n    return f1_score(all_labels, all_preds)\n\n# Анализ влияния разреженности на F1\n@torch.no_grad()\ndef sensitivity_analysis_f1(model, dataloader, sparsity_values):\n    \"\"\"\n    Проводит анализ влияния разреженности слоев модели на F1-метрику.\n    \"\"\"\n    # Сохраняем исходное состояние параметров\n    original_state = {name: param.clone() for name, param in model.named_parameters() if param.dim() > 1}\n\n    sensitivity_results = {}\n    for name, param in tqdm(model.named_parameters(), desc=\"Analyzing Layers\"):\n        if param.dim() > 1:  # Только для весов с размерностью больше 1\n            f1_scores = []\n            for sparsity in sparsity_values:\n                # Разрежаем слой\n                print(f\"Pruning {name} with sparsity {sparsity}...\")\n                fine_grained_prune(param, sparsity)\n                # Оцениваем F1\n                f1 = evaluate_model_f1(model, dataloader)\n                f1_scores.append(f1)\n                print(f\"F1 Score after pruning {name} at sparsity {sparsity}: {f1:.4f}\")\n                # Восстанавливаем параметры слоя\n                param.data.copy_(original_state[name])\n\n            sensitivity_results[name] = f1_scores\n            print(f\"Layer {name} analysis completed.\")\n    return sensitivity_results\n\n# Настройки анализа\nsparsity_values = [0.1, 0.3, 0.5, 0.7, 0.9]  # Уровни разреженности\n# Уменьшение объема данных для теста\nsmall_val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Запуск анализа\nsensitivity_results_f1 = sensitivity_analysis_f1(discriminator, small_val_loader, sparsity_values)\n\n# Визуализация результатов\nimport matplotlib.pyplot as plt\n\nfor layer_name, f1_scores in sensitivity_results_f1.items():\n    plt.plot(sparsity_values, f1_scores, label=layer_name)\n\nplt.xlabel(\"Sparsity\")\nplt.ylabel(\"F1 Score\")\nplt.title(\"Sensitivity Analysis: Layer Sparsity vs F1 Score\")\nplt.legend()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:24:12.520440Z","iopub.execute_input":"2024-12-09T09:24:12.520891Z","iopub.status.idle":"2024-12-09T09:29:16.637994Z","shell.execute_reply.started":"2024-12-09T09:24:12.520857Z","shell.execute_reply":"2024-12-09T09:29:16.637044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EER","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_curve\n\n# Функция для применения разреженности к отдельному слою\n@torch.no_grad()\ndef fine_grained_prune(param, sparsity):\n    \"\"\"\n    Применяет разреженность (прунинг) к параметру слоя.\n    \"\"\"\n    num_params = param.numel()\n    num_zero = int(num_params * sparsity)  # Количество обнуляемых весов\n\n    # Определяем пороговое значение для обнуления\n    threshold = torch.topk(param.abs().view(-1), num_zero, largest=False).values.max()\n\n    # Создаем маску и применяем разрежение\n    mask = (param.abs() > threshold).float()\n    param.mul_(mask)  # Применяем маску in-place\n    return mask\n\n# Функция вычисления EER\ndef calculate_eer(y_true, y_pred):\n    \"\"\"\n    Вычисляет Equal Error Rate (EER).\n    \"\"\"\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    fnr = 1 - tpr  # False Negative Rate\n    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n    return eer\n\n# Функция оценки EER\ndef evaluate_model_eer(model, dataloader):\n    model.eval()\n    all_labels = []\n    all_scores = []\n    with torch.no_grad():\n        for data, labels in tqdm(dataloader, desc=\"Evaluating Model\", leave=False):\n            data, labels = data.to(device), labels.to(device)\n            outputs = model(data)\n            all_scores.extend(outputs.cpu().numpy().flatten())\n            all_labels.extend(labels.cpu().numpy())\n    return calculate_eer(all_labels, all_scores)\n\n# Анализ влияния разреженности на EER\n@torch.no_grad()\ndef sensitivity_analysis_eer(model, dataloader, sparsity_values):\n    \"\"\"\n    Проводит анализ влияния разреженности слоев модели на EER.\n    \"\"\"\n    # Сохраняем исходное состояние параметров\n    original_state = {name: param.clone() for name, param in model.named_parameters() if param.dim() > 1}\n\n    sensitivity_results = {}\n    for name, param in tqdm(model.named_parameters(), desc=\"Analyzing Layers\"):\n        if param.dim() > 1:  # Только для весов с размерностью больше 1\n            eers = []\n            for sparsity in sparsity_values:\n                # Разрежаем слой\n                print(f\"Pruning {name} with sparsity {sparsity}...\")\n                fine_grained_prune(param, sparsity)\n                # Оцениваем EER\n                eer = evaluate_model_eer(model, dataloader)\n                eers.append(eer)\n                print(f\"EER after pruning {name} at sparsity {sparsity}: {eer:.4f}\")\n                # Восстанавливаем параметры слоя\n                param.data.copy_(original_state[name])\n\n            sensitivity_results[name] = eers\n            print(f\"Layer {name} analysis completed.\")\n    return sensitivity_results\n\n# Настройки анализа\nsparsity_values = [0.1, 0.3, 0.5, 0.7, 0.9]  # Уровни разреженности\n# Уменьшение объема данных для теста\nsmall_val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Запуск анализа\nsensitivity_results_eer = sensitivity_analysis_eer(discriminator, small_val_loader, sparsity_values)\n\n# Визуализация результатов\nimport matplotlib.pyplot as plt\n\nfor layer_name, eers in sensitivity_results_eer.items():\n    plt.plot(sparsity_values, eers, label=layer_name)\n\nplt.xlabel(\"Sparsity\")\nplt.ylabel(\"EER\")\nplt.title(\"Sensitivity Analysis: Layer Sparsity vs EER\")\nplt.legend()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:35:11.440839Z","iopub.execute_input":"2024-12-12T14:35:11.441715Z","iopub.status.idle":"2024-12-12T14:35:19.586061Z","shell.execute_reply.started":"2024-12-12T14:35:11.441667Z","shell.execute_reply":"2024-12-12T14:35:19.584714Z"}},"outputs":[{"name":"stderr","text":"Analyzing Layers: 0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Pruning model.0.weight with sparsity 0.1...\n","output_type":"stream"},{"name":"stderr","text":"\nEvaluating Model:   0%|          | 0/1553 [00:00<?, ?it/s]\u001b[A\nEvaluating Model:   0%|          | 1/1553 [00:00<10:57,  2.36it/s]\u001b[A\nEvaluating Model:   0%|          | 2/1553 [00:00<06:21,  4.07it/s]\u001b[A\nEvaluating Model:   0%|          | 3/1553 [00:00<04:47,  5.39it/s]\u001b[A\nEvaluating Model:   0%|          | 4/1553 [00:00<04:09,  6.20it/s]\u001b[A\nEvaluating Model:   0%|          | 5/1553 [00:00<03:46,  6.84it/s]\u001b[A\nEvaluating Model:   0%|          | 6/1553 [00:01<03:27,  7.44it/s]\u001b[A\nEvaluating Model:   0%|          | 7/1553 [00:01<03:15,  7.91it/s]\u001b[A\nEvaluating Model:   1%|          | 8/1553 [00:01<03:07,  8.25it/s]\u001b[A\nEvaluating Model:   1%|          | 9/1553 [00:01<03:03,  8.40it/s]\u001b[A\nEvaluating Model:   1%|          | 10/1553 [00:01<03:00,  8.55it/s]\u001b[A\nEvaluating Model:   1%|          | 11/1553 [00:01<02:58,  8.63it/s]\u001b[A\nEvaluating Model:   1%|          | 12/1553 [00:01<02:56,  8.72it/s]\u001b[A\nEvaluating Model:   1%|          | 13/1553 [00:02<04:35,  5.59it/s]\u001b[A\nEvaluating Model:   1%|          | 14/1553 [00:02<05:59,  4.29it/s]\u001b[A\nEvaluating Model:   1%|          | 15/1553 [00:02<05:02,  5.08it/s]\u001b[A\nEvaluating Model:   1%|          | 16/1553 [00:02<04:22,  5.86it/s]\u001b[A\nEvaluating Model:   1%|          | 17/1553 [00:02<03:52,  6.61it/s]\u001b[A\nEvaluating Model:   1%|          | 18/1553 [00:02<03:34,  7.16it/s]\u001b[A\nEvaluating Model:   1%|          | 19/1553 [00:02<03:36,  7.10it/s]\u001b[A\nEvaluating Model:   1%|▏         | 20/1553 [00:03<03:26,  7.44it/s]\u001b[A\nEvaluating Model:   1%|▏         | 21/1553 [00:03<03:31,  7.24it/s]\u001b[A\nEvaluating Model:   1%|▏         | 22/1553 [00:03<03:18,  7.73it/s]\u001b[A\nEvaluating Model:   1%|▏         | 23/1553 [00:03<03:08,  8.11it/s]\u001b[A\nEvaluating Model:   2%|▏         | 24/1553 [00:03<02:59,  8.51it/s]\u001b[A\nEvaluating Model:   2%|▏         | 25/1553 [00:03<02:57,  8.61it/s]\u001b[A\nEvaluating Model:   2%|▏         | 26/1553 [00:03<03:30,  7.27it/s]\u001b[A\nEvaluating Model:   2%|▏         | 27/1553 [00:04<05:09,  4.93it/s]\u001b[A\nEvaluating Model:   2%|▏         | 28/1553 [00:04<05:03,  5.02it/s]\u001b[A\nEvaluating Model:   2%|▏         | 29/1553 [00:04<04:20,  5.86it/s]\u001b[A\nEvaluating Model:   2%|▏         | 30/1553 [00:04<03:51,  6.57it/s]\u001b[A\nEvaluating Model:   2%|▏         | 31/1553 [00:04<03:32,  7.16it/s]\u001b[A\nEvaluating Model:   2%|▏         | 32/1553 [00:04<03:18,  7.67it/s]\u001b[A\nEvaluating Model:   2%|▏         | 33/1553 [00:04<03:09,  8.02it/s]\u001b[A\nEvaluating Model:   2%|▏         | 34/1553 [00:05<03:04,  8.22it/s]\u001b[A\nEvaluating Model:   2%|▏         | 35/1553 [00:05<02:59,  8.45it/s]\u001b[A\nEvaluating Model:   2%|▏         | 36/1553 [00:05<02:58,  8.51it/s]\u001b[A\nEvaluating Model:   2%|▏         | 37/1553 [00:05<02:53,  8.75it/s]\u001b[A\nEvaluating Model:   2%|▏         | 38/1553 [00:05<02:58,  8.48it/s]\u001b[A\nEvaluating Model:   3%|▎         | 39/1553 [00:05<04:32,  5.55it/s]\u001b[A\nEvaluating Model:   3%|▎         | 40/1553 [00:06<05:53,  4.28it/s]\u001b[A\nEvaluating Model:   3%|▎         | 41/1553 [00:06<04:58,  5.07it/s]\u001b[A\nEvaluating Model:   3%|▎         | 42/1553 [00:06<04:21,  5.77it/s]\u001b[A\nEvaluating Model:   3%|▎         | 43/1553 [00:06<03:56,  6.38it/s]\u001b[A\nEvaluating Model:   3%|▎         | 44/1553 [00:06<03:42,  6.78it/s]\u001b[A\nEvaluating Model:   3%|▎         | 45/1553 [00:06<03:28,  7.23it/s]\u001b[A\nEvaluating Model:   3%|▎         | 46/1553 [00:06<03:19,  7.55it/s]\u001b[A\nEvaluating Model:   3%|▎         | 47/1553 [00:07<03:12,  7.84it/s]\u001b[A\nEvaluating Model:   3%|▎         | 48/1553 [00:07<03:06,  8.09it/s]\u001b[A\nEvaluating Model:   3%|▎         | 49/1553 [00:07<03:04,  8.16it/s]\u001b[A\nEvaluating Model:   3%|▎         | 50/1553 [00:07<04:32,  5.52it/s]\u001b[A\nAnalyzing Layers: 0it [00:07, ?it/s]                               \u001b[A\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m small_val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Запуск анализа\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m sensitivity_results_eer \u001b[38;5;241m=\u001b[39m \u001b[43msensitivity_analysis_eer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmall_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Визуализация результатов\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 64\u001b[0m, in \u001b[0;36msensitivity_analysis_eer\u001b[0;34m(model, dataloader, sparsity_values)\u001b[0m\n\u001b[1;32m     62\u001b[0m fine_grained_prune(param, sparsity)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Оцениваем EER\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m eer \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_eer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m eers\u001b[38;5;241m.\u001b[39mappend(eer)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEER after pruning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at sparsity \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparsity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meer\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[31], line 39\u001b[0m, in \u001b[0;36mevaluate_model_eer\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     37\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Model\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     40\u001b[0m         data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(data)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Загружаем и обрабатываем аудио\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_length:\n\u001b[1;32m     17\u001b[0m     audio \u001b[38;5;241m=\u001b[39m audio[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_length]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m         y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    206\u001b[0m     context \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n\u001b[1;32m    212\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m sf_desc\u001b[38;5;241m.\u001b[39msamplerate\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/soundfile.py:1205\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1204\u001b[0m             file \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mencode(_sys\u001b[38;5;241m.\u001b[39mgetfilesystemencoding())\n\u001b[0;32m-> 1205\u001b[0m     file_ptr \u001b[38;5;241m=\u001b[39m \u001b[43mopenfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1207\u001b[0m     file_ptr \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_open_fd(file, mode_int, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info, closefd)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":31},{"cell_type":"markdown","source":"### ПРИМЕНЯЕМ ПРУНИНГ И СОХРАНЯЕМ МОДЕЛЬ!","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Функция для применения разреженности (прунинга)\n@torch.no_grad()\ndef apply_pruning(model, sparsity_dict):\n    \"\"\"\n    Применяет прунинг к модели на основе заданного словаря разреженности.\n    \"\"\"\n    for name, param in model.named_parameters():\n        if name in sparsity_dict and param.dim() > 1:\n            # Применяем fine-grained разреженность к указанному слою\n            fine_grained_prune(param, sparsity_dict[name])\n\n# Функция для сохранения модели после прунинга\ndef save_pruned_model(model, save_path):\n    \"\"\"\n    Сохраняет веса модели после прунинга.\n    \"\"\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Pruned model saved to {save_path}\")\n\n# Пример задания разреженности для каждого слоя модели\nsparsity_dict = {\n    'model.0.weight': 0.05,  # 20% разреженность\n    'model.3.weight': 0.05,  # 30% разреженность\n    'model.7.weight': 0.5,  # 50% разреженность\n    'model.11.weight': 0.1, # 10% разреженность\n    'fc.0.weight': 0.7      # 70% разреженность \n}\n\n# Применяем разреженность к модели\napply_pruning(discriminator, sparsity_dict)\n\n# Сохраняем веса модели после прунинга\nsave_path = \"pruned_discriminator.pth\"\nsave_pruned_model(discriminator, save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:06:48.560366Z","iopub.execute_input":"2024-12-09T10:06:48.561240Z","iopub.status.idle":"2024-12-09T10:06:48.599676Z","shell.execute_reply.started":"2024-12-09T10:06:48.561203Z","shell.execute_reply":"2024-12-09T10:06:48.598750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Функция для подсчета параметров модели\ndef count_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    zero_params = sum((p == 0).sum().item() for p in model.parameters() if p.dim() > 1)\n    nonzero_params = total_params - zero_params\n    return total_params, zero_params, nonzero_params\n\n# Функция для применения разреженности (прунинга)\n@torch.no_grad()\ndef apply_pruning(model, sparsity_dict):\n    \"\"\"\n    Применяет прунинг к модели на основе заданного словаря разреженности.\n    \"\"\"\n    for name, param in model.named_parameters():\n        if name in sparsity_dict and param.dim() > 1:\n            # Применяем fine-grained разреженность к указанному слою\n            fine_grained_prune(param, sparsity_dict[name])\n\n# Функция для сохранения модели после прунинга\ndef save_pruned_model(model, save_path):\n    \"\"\"\n    Сохраняет веса модели после прунинга.\n    \"\"\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Pruned model saved to {save_path}\")\n\n# Пример задания разреженности для каждого слоя модели\nsparsity_dict = {\n    'model.0.weight': 0.05,  # 20% разреженность\n    'model.3.weight': 0.05,  # 30% разреженность\n    'model.7.weight': 0.5,  # 50% разреженность\n    'model.11.weight': 0.3, # 10% разреженность\n    'fc.0.weight': 0.7      # 70% разреженность \n}\n\n# Подсчет параметров до прунинга\ntotal_params_before, zero_params_before, nonzero_params_before = count_parameters(discriminator)\nprint(f\"Before pruning: Total parameters = {total_params_before}, Nonzero = {nonzero_params_before}, Zero = {zero_params_before}\")\n\n# Применяем разреженность к модели\napply_pruning(discriminator, sparsity_dict)\n\n# Подсчет параметров после прунинга\ntotal_params_after, zero_params_after, nonzero_params_after = count_parameters(discriminator)\nprint(f\"After pruning: Total parameters = {total_params_after}, Nonzero = {nonzero_params_after}, Zero = {zero_params_after}\")\n\n# Степень разреженности модели\nsparsity_ratio = zero_params_after / total_params_after * 100\nprint(f\"Model sparsity: {sparsity_ratio:.2f}%\")\n\n# Сохраняем веса модели после прунинга\nsave_path = \"pruned_discriminator3.pth\"\nsave_pruned_model(discriminator, save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:35:32.085004Z","iopub.execute_input":"2024-12-12T14:35:32.085338Z","iopub.status.idle":"2024-12-12T14:35:32.179616Z","shell.execute_reply.started":"2024-12-12T14:35:32.085306Z","shell.execute_reply":"2024-12-12T14:35:32.178705Z"}},"outputs":[{"name":"stdout","text":"Before pruning: Total parameters = 4434177, Nonzero = 4434017, Zero = 160\nAfter pruning: Total parameters = 4434177, Nonzero = 2940821, Zero = 1493356\nModel sparsity: 33.68%\nPruned model saved to pruned_discriminator3.pth\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Путь\nsave_path = \"/kaggle/working/pruned_discriminator.pth\"\n\n\ndisplay(FileLink(\"pruned_discriminator.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:27:45.110595Z","iopub.execute_input":"2024-12-09T10:27:45.110976Z","iopub.status.idle":"2024-12-09T10:27:45.117238Z","shell.execute_reply.started":"2024-12-09T10:27:45.110945Z","shell.execute_reply":"2024-12-09T10:27:45.116182Z"}},"outputs":[],"execution_count":null}]}